#!/usr/bin/env python
# -*- coding: utf-8 -*-

import codecs
import copy
import csv
import json
import logging
import logging.config
import math
import os
import random
import re
import sqlite3
import sys
import time
import warnings
import webbrowser
from collections import OrderedDict
from datetime import date, datetime, timedelta
from pathlib import Path
from time import sleep

import requests
from requests.exceptions import RequestException
from lxml import etree
import json5
from requests.adapters import HTTPAdapter
from tqdm import tqdm

import const
from util import csvutil
from util.dateutil import convert_to_days_ago
from util.notify import push_deer
from util.llm_analyzer import LLMAnalyzer  # å¯¼å…¥ LLM åˆ†æå™¨

import piexif

warnings.filterwarnings("ignore")

# å¦‚æœæ—¥å¿—æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»º
if not os.path.isdir("log/"):
    os.makedirs("log/")
logging_path = os.path.split(os.path.realpath(__file__))[0] + os.sep + "logging.conf"
logging.config.fileConfig(logging_path)
logger = logging.getLogger("weibo")

# æ—¥æœŸæ—¶é—´æ ¼å¼
DTFORMAT = "%Y-%m-%dT%H:%M:%S"

class Weibo(object):
    def __init__(self, config):
        """Weiboç±»åˆå§‹åŒ–"""
        self.validate_config(config)
        self.only_crawl_original = config["only_crawl_original"]  # å–å€¼èŒƒå›´ä¸º0ã€1,ç¨‹åºé»˜è®¤å€¼ä¸º0,ä»£è¡¨è¦çˆ¬å–ç”¨æˆ·çš„å…¨éƒ¨å¾®åš,1ä»£è¡¨åªçˆ¬å–ç”¨æˆ·çš„åŸåˆ›å¾®åš
        self.remove_html_tag = config[
            "remove_html_tag"
        ]  # å–å€¼èŒƒå›´ä¸º0ã€1, 0ä»£è¡¨ä¸ç§»é™¤å¾®åšä¸­çš„html tag, 1ä»£è¡¨ç§»é™¤
        since_date = config["since_date"]
        # since_date è‹¥ä¸ºæ•´æ•°ï¼Œåˆ™å–è¯¥å¤©æ•°ä¹‹å‰çš„æ—¥æœŸï¼›è‹¥ä¸º yyyy-mm-ddï¼Œåˆ™å¢åŠ æ—¶é—´
        if isinstance(since_date, int):
            since_date = date.today() - timedelta(since_date)
            since_date = since_date.strftime(DTFORMAT)
        elif self.is_date(since_date):
            since_date = "{}T00:00:00".format(since_date)
        elif self.is_datetime(since_date):
            pass
        else:
            logger.error("since_date æ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·ç¡®è®¤é…ç½®æ˜¯å¦æ­£ç¡®")
            sys.exit()
        self.since_date = since_date  # èµ·å§‹æ—¶é—´ï¼Œå³çˆ¬å–å‘å¸ƒæ—¥æœŸä»è¯¥å€¼åˆ°ç°åœ¨çš„å¾®åšï¼Œå½¢å¼ä¸ºyyyy-mm-ddThh:mm:ssï¼Œå¦‚ï¼š2023-08-21T09:23:03
        self.start_page = config.get("start_page", 1)  # å¼€å§‹çˆ¬çš„é¡µï¼Œå¦‚æœä¸­é€”è¢«é™åˆ¶è€Œç»“æŸå¯ä»¥ç”¨æ­¤å®šä¹‰å¼€å§‹é¡µç 
        self.write_mode = config[
            "write_mode"
        ]  # ç»“æœä¿¡æ¯ä¿å­˜ç±»å‹ï¼Œä¸ºlistå½¢å¼ï¼Œå¯åŒ…å«csvã€mongoå’Œmysqlä¸‰ç§ç±»å‹
        self.original_pic_download = config[
            "original_pic_download"
        ]  # å–å€¼èŒƒå›´ä¸º0ã€1, 0ä»£è¡¨ä¸ä¸‹è½½åŸåˆ›å¾®åšå›¾ç‰‡,1ä»£è¡¨ä¸‹è½½
        self.retweet_pic_download = config[
            "retweet_pic_download"
        ]  # å–å€¼èŒƒå›´ä¸º0ã€1, 0ä»£è¡¨ä¸ä¸‹è½½è½¬å‘å¾®åšå›¾ç‰‡,1ä»£è¡¨ä¸‹è½½
        self.original_video_download = config[
            "original_video_download"
        ]  # å–å€¼èŒƒå›´ä¸º0ã€1, 0ä»£è¡¨ä¸ä¸‹è½½åŸåˆ›å¾®åšè§†é¢‘,1ä»£è¡¨ä¸‹è½½
        self.retweet_video_download = config[
            "retweet_video_download"
        ]  # å–å€¼èŒƒå›´ä¸º0ã€1, 0ä»£è¡¨ä¸ä¸‹è½½è½¬å‘å¾®åšè§†é¢‘,1ä»£è¡¨ä¸‹è½½
        
        # æ–°å¢Live Photoè§†é¢‘ä¸‹è½½é…ç½®
        self.original_live_photo_download = config.get("original_live_photo_download", 0)
        self.retweet_live_photo_download = config.get("retweet_live_photo_download", 0)
        
        self.download_comment = config["download_comment"]  # 1ä»£è¡¨ä¸‹è½½è¯„è®º,0ä»£è¡¨ä¸ä¸‹è½½
        self.comment_max_download_count = config[
            "comment_max_download_count"
        ]  # å¦‚æœè®¾ç½®äº†ä¸‹è¯„è®ºï¼Œæ¯æ¡å¾®åšè¯„è®ºæ•°ä¼šé™åˆ¶åœ¨è¿™ä¸ªå€¼å†…
        self.download_repost = config["download_repost"]  # 1ä»£è¡¨ä¸‹è½½è½¬å‘,0ä»£è¡¨ä¸ä¸‹è½½
        self.repost_max_download_count = config[
            "repost_max_download_count"
        ]  # å¦‚æœè®¾ç½®äº†ä¸‹è½¬å‘ï¼Œæ¯æ¡å¾®åšè½¬å‘æ•°ä¼šé™åˆ¶åœ¨è¿™ä¸ªå€¼å†…
        self.user_id_as_folder_name = config.get(
            "user_id_as_folder_name", 0
        )  # ç»“æœç›®å½•åï¼Œå–å€¼ä¸º0æˆ–1ï¼Œå†³å®šç»“æœæ–‡ä»¶å­˜å‚¨åœ¨ç”¨æˆ·æ˜µç§°æ–‡ä»¶å¤¹é‡Œè¿˜æ˜¯ç”¨æˆ·idæ–‡ä»¶å¤¹é‡Œ
        self.write_time_in_exif = config.get(
            "write_time_in_exif", 0
        )  # æ˜¯å¦å¼€å¯å¾®åšæ—¶é—´å†™å…¥EXIFï¼Œå–å€¼èŒƒå›´ä¸º0ã€1, 0ä»£è¡¨ä¸å¼€å¯, 1ä»£è¡¨å¼€å¯
        self.change_file_time = config.get(
            "change_file_time", 0
        )  # æ˜¯å¦ä¿®æ”¹æ–‡ä»¶æ—¶é—´ï¼Œå–å€¼èŒƒå›´ä¸º0ã€1, 0ä»£è¡¨ä¸å¼€å¯, 1ä»£è¡¨å¼€å¯
        
        # Cookieæ”¯æŒï¼šä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡WEIBO_COOKIEï¼Œå…¶æ¬¡ä½¿ç”¨config.jsonä¸­çš„é…ç½®
        cookie_string = os.environ.get("WEIBO_COOKIE") or config.get("cookie")
        if os.environ.get("WEIBO_COOKIE"):
            logger.info("ä½¿ç”¨ç¯å¢ƒå˜é‡WEIBO_COOKIEä¸­çš„Cookie")
        
        core_cookies = {}   # æ ¸å¿ƒåŒ…
        backup_cookies = {} # å¤‡ä»½
        # Cookieæ¸…æ´—ï¼šæå–æ ¸å¿ƒå­—æ®µã€‚è‹¥åç»­é¢„çƒ­å¤±è´¥ï¼Œåˆ™å›é€€ä½¿ç”¨åŸç‰ˆ _T_WM/XSRF-TOKEN
        if cookie_string and "SUB=" in cookie_string:
            # 1. æå–æ ¸å¿ƒ SUB
            match_sub = re.search(r'SUB=(.*?)(;|$)', cookie_string)
            if match_sub:
                core_cookies['SUB'] = match_sub.group(1)
            
            # 2. æå–å¤‡ä»½æŒ‡çº¹
            match_twm = re.search(r'_T_WM=(.*?)(;|$)', cookie_string)
            if match_twm:
                backup_cookies['_T_WM'] = match_twm.group(1)
            
            match_xsrf = re.search(r'XSRF-TOKEN=(.*?)(;|$)', cookie_string)
            if match_xsrf:
                backup_cookies['XSRF-TOKEN'] = match_xsrf.group(1)
        
        # ä¿åº•ï¼šå¦‚æœæ²¡æœ‰æå–åˆ° SUBï¼Œè¯´æ˜æ ¼å¼ç‰¹æ®Šï¼Œå…¨é‡åŠ è½½
        if not core_cookies and cookie_string:
            for pair in cookie_string.split(';'):
                if '=' in pair:
                    key, value = pair.split('=', 1)
                    core_cookies[key.strip()] = value.strip()
                    
        self.headers = {
            'Referer': 'https://m.weibo.cn/',  # ä¿®æ­£ Referer ä¸º m.weibo.cn
            'accept': 'application/json, text/plain, */*',
            'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
            'cache-control': 'max-age=0',
            'priority': 'u=0, i',
            'sec-ch-ua': '"Chromium";v="136", "Microsoft Edge";v="136", "Not.A/Brand";v="99"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"',
            'sec-fetch-dest': 'empty',
            'sec-fetch-mode': 'navigate',
            'sec-fetch-site': 'same-origin',
            'upgrade-insecure-requests': '1',
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36 Edg/136.0.0.0',
        }
        self.mysql_config = config.get("mysql_config")  # MySQLæ•°æ®åº“è¿æ¥é…ç½®ï¼Œå¯ä»¥ä¸å¡«
        self.mongodb_URI = config.get("mongodb_URI")  # MongoDBæ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²ï¼Œå¯ä»¥ä¸å¡«
        self.post_config = config.get("post_config")  # post_configï¼Œå¯ä»¥ä¸å¡«
        self.page_weibo_count = config.get("page_weibo_count")  # page_weibo_countï¼Œçˆ¬å–ä¸€é¡µçš„å¾®åšæ•°ï¼Œé»˜è®¤10é¡µ
        
        # åˆå§‹åŒ– LLM åˆ†æå™¨
        self.llm_analyzer = LLMAnalyzer(config) if config.get("llm_config") else None
        
        user_id_list = config["user_id_list"]
        requests_session = requests.Session()
        requests_session.cookies.update(core_cookies)

        self.session = requests_session
        try:
            # è¯·æ±‚åªå¸¦ SUB
            # æœåŠ¡å™¨ä¸‹å‘é€‚é… m.weibo.cn çš„æ–°æŒ‡çº¹
            self.session.get("https://m.weibo.cn", headers=self.headers, timeout=10)
            logger.info("Session é¢„çƒ­æˆåŠŸï¼ŒæœåŠ¡å™¨å·²ä¸‹å‘æœ€æ–°æŒ‡çº¹ã€‚")
            
        except Exception as e:
            #è¯·æ±‚å¤±è´¥æ—¶ï¼Œå¯ç”¨å¤‡ä»½
            logger.warning(f"Session é¢„çƒ­å¤±è´¥ ({e})ï¼Œæ­£åœ¨å¯ç”¨å¤‡ä»½ Cookie...")
            self.session.cookies.update(backup_cookies) # æŠŠæ—§æŒ‡çº¹è£…è¿›å»æ•‘æ€¥

        adapter = HTTPAdapter(max_retries=5)
        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)
        # é¿å…å¡ä½
        if isinstance(user_id_list, list):
            random.shuffle(user_id_list)

        query_list = config.get("query_list") or []
        if isinstance(query_list, str):
            query_list = query_list.split(",")
        self.query_list = query_list
        if not isinstance(user_id_list, list):
            if not os.path.isabs(user_id_list):
                user_id_list = (
                    os.path.split(os.path.realpath(__file__))[0] + os.sep + user_id_list
                )
            self.user_config_file_path = user_id_list  # ç”¨æˆ·é…ç½®æ–‡ä»¶è·¯å¾„
            user_config_list = self.get_user_config_list(user_id_list)
        else:
            self.user_config_file_path = ""
            user_config_list = [
                {
                    "user_id": user_id,
                    "since_date": self.since_date,
                    "query_list": query_list,
                }
                for user_id in user_id_list
            ]

        self.user_config_list = user_config_list  # è¦çˆ¬å–çš„å¾®åšç”¨æˆ·çš„user_configåˆ—è¡¨
        self.user_config = {}  # ç”¨æˆ·é…ç½®,åŒ…å«ç”¨æˆ·idå’Œsince_date
        self.start_date = ""  # è·å–ç”¨æˆ·ç¬¬ä¸€æ¡å¾®åšæ—¶çš„æ—¥æœŸ
        self.query = ""
        self.user = {}  # å­˜å‚¨ç›®æ ‡å¾®åšç”¨æˆ·ä¿¡æ¯
        self.got_count = 0  # å­˜å‚¨çˆ¬å–åˆ°çš„å¾®åšæ•°
        self.weibo = []  # å­˜å‚¨çˆ¬å–åˆ°çš„æ‰€æœ‰å¾®åšä¿¡æ¯
        self.weibo_id_list = []  # å­˜å‚¨çˆ¬å–åˆ°çš„æ‰€æœ‰å¾®åšid
        self.long_sleep_count_before_each_user = 0 #æ¯ä¸ªç”¨æˆ·å‰çš„é•¿æ—¶é—´sleepé¿å…è¢«ban
        self.store_binary_in_sqlite = config.get("store_binary_in_sqlite", 0)

        # é˜²å°ç¦é…ç½®åˆå§‹åŒ–
        self.anti_ban_config = config.get("anti_ban_config", {})
        self.anti_ban_enabled = self.anti_ban_config.get("enabled", False)

        # çˆ¬å–çŠ¶æ€è·Ÿè¸ª
        self.crawl_stats = {
            "weibo_count": 0,      # å·²çˆ¬å–å¾®åšæ•°
            "request_count": 0,    # å·²å‘é€è¯·æ±‚æ•°
            "api_errors": 0,       # APIé”™è¯¯æ•°
            "start_time": None,    # å¼€å§‹æ—¶é—´
            "batch_count": 0,      # å½“å‰æ‰¹æ¬¡è®¡æ•°
            "last_batch_time": None # ä¸Šæ¬¡æ‰¹æ¬¡æ—¶é—´
        }
    def calculate_dynamic_delay(self):
        """è®¡ç®—åŠ¨æ€å»¶è¿Ÿæ—¶é—´"""
        if not self.anti_ban_enabled:
            return 0

        config = self.anti_ban_config
        base_delay = config.get("request_delay_min", 8)

        # æ ¹æ®è¯·æ±‚æ¬¡æ•°å¢åŠ å»¶è¿Ÿ
        request_count = self.crawl_stats["request_count"]
        if request_count > 100:
            base_delay += 5
        if request_count > 300:
            base_delay += 10

        # æ ¹æ®çˆ¬å–æ—¶é—´å¢åŠ å»¶è¿Ÿ
        if self.crawl_stats["start_time"]:
            time_elapsed = time.time() - self.crawl_stats["start_time"]
            if time_elapsed > 300:  # 5åˆ†é’Ÿ
                base_delay += 5

        # éšæœºæ³¢åŠ¨
        max_delay = config.get("request_delay_max", 15)
        return random.uniform(base_delay, max_delay)

    def should_pause_session(self):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æš‚åœå½“å‰ä¼šè¯"""
        if not self.anti_ban_enabled:
            return False, ""

        config = self.anti_ban_config
        current_time = time.time()

        # æ¡ä»¶1ï¼šè¾¾åˆ°æ•°é‡é˜ˆå€¼
        max_weibo = config.get("max_weibo_per_session", 500)
        if self.crawl_stats["weibo_count"] >= max_weibo:
            return True, f"è¾¾åˆ°å•æ¬¡è¿è¡Œæœ€å¤§å¾®åšæ•°({max_weibo})"

        # æ¡ä»¶2ï¼šè¿è¡Œæ—¶é—´è¿‡é•¿
        if self.crawl_stats["start_time"]:
            session_time = current_time - self.crawl_stats["start_time"]
            max_time = config.get("max_session_time", 600)
            if session_time > max_time:
                return True, f"å•æ¬¡è¿è¡Œæ—¶é—´è¿‡é•¿({int(session_time)}ç§’)"

        # æ¡ä»¶3ï¼šAPIé”™è¯¯ç‡è¿‡é«˜
        max_errors = config.get("max_api_errors", 5)
        if self.crawl_stats["api_errors"] >= max_errors:
            return True, f"APIé”™è¯¯è¿‡å¤š({self.crawl_stats['api_errors']}æ¬¡)"

        # æ¡ä»¶4ï¼šéšæœºæ¦‚ç‡ï¼ˆæ¨¡æ‹Ÿç”¨æˆ·ä¼‘æ¯ï¼‰
        random_prob = config.get("random_rest_probability", 0.01)
        if random.random() < random_prob:
            return True, "éšæœºä¼‘æ¯"

        return False, ""

    def check_batch_delay(self):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰¹æ¬¡å»¶è¿Ÿ"""
        if not self.anti_ban_enabled:
            return

        config = self.anti_ban_config
        batch_size = config.get("batch_size", 50)
        batch_delay = config.get("batch_delay", 30)

        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ‰¹æ¬¡å¤§å°
        if self.crawl_stats["batch_count"] >= batch_size:
            current_time = time.time()

            # æ£€æŸ¥è·ç¦»ä¸Šæ¬¡æ‰¹æ¬¡çš„æ—¶é—´
            if self.crawl_stats["last_batch_time"]:
                time_since_last_batch = current_time - self.crawl_stats["last_batch_time"]
                if time_since_last_batch < batch_delay:
                    # å¦‚æœè·ç¦»ä¸Šæ¬¡æ‰¹æ¬¡æ—¶é—´å¤ªçŸ­ï¼Œç­‰å¾…è¡¥è¶³
                    wait_time = batch_delay - time_since_last_batch
                    logger.info(f"æ‰¹æ¬¡å»¶è¿Ÿ: ç­‰å¾… {wait_time:.1f} ç§’")
                    sleep(wait_time)

            logger.info(f"æ‰¹æ¬¡å»¶è¿Ÿ: ç­‰å¾… {batch_delay} ç§’")
            sleep(batch_delay)

            # é‡ç½®æ‰¹æ¬¡è®¡æ•°
            self.crawl_stats["batch_count"] = 0
            self.crawl_stats["last_batch_time"] = time.time()

    def get_random_headers(self):
        """è·å–éšæœºè¯·æ±‚å¤´"""
        if not self.anti_ban_enabled:
            return self.headers

        config = self.anti_ban_config

        # éšæœºé€‰æ‹©User-Agent
        user_agents = config.get("user_agents", [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36 Edg/136.0.0.0"
        ])
        user_agent = random.choice(user_agents)

        # éšæœºé€‰æ‹©Accept-Language
        accept_languages = config.get("accept_languages", [
            "zh-CN,zh;q=0.9,en;q=0.8"
        ])
        accept_language = random.choice(accept_languages)

        # éšæœºé€‰æ‹©Referer
        referers = config.get("referer_list", [
            "https://m.weibo.cn/",
            "https://weibo.com/"
        ])
        referer = random.choice(referers)

        # è¿”å›éšæœºåŒ–çš„è¯·æ±‚å¤´
        return {
            'Referer': referer,
            'accept': 'application/json, text/plain, */*',
            'accept-language': accept_language,
            'cache-control': 'max-age=0',
            'priority': 'u=0, i',
            'sec-ch-ua': '"Chromium";v="136", "Microsoft Edge";v="136", "Not.A/Brand";v="99"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"',
            'sec-fetch-dest': 'empty',
            'sec-fetch-mode': 'navigate',
            'sec-fetch-site': 'same-origin',
            'upgrade-insecure-requests': '1',
            'user-agent': user_agent,
        }

    def update_crawl_stats(self, weibo_count=0, request_count=0, api_error=False):
        """æ›´æ–°çˆ¬å–ç»Ÿè®¡"""
        if not self.anti_ban_enabled:
            return

        if weibo_count > 0:
            self.crawl_stats["weibo_count"] += weibo_count
            self.crawl_stats["batch_count"] += weibo_count

        if request_count > 0:
            self.crawl_stats["request_count"] += request_count

        if api_error:
            self.crawl_stats["api_errors"] += 1

    def reset_crawl_stats(self):
        """é‡ç½®çˆ¬å–ç»Ÿè®¡ï¼ˆä¼‘æ¯åè°ƒç”¨ï¼‰"""
        self.crawl_stats = {
            "weibo_count": 0,
            "request_count": 0,
            "api_errors": 0,
            "start_time": time.time(),
            "batch_count": 0,
            "last_batch_time": None
        }
        logger.info("çˆ¬å–ç»Ÿè®¡å·²é‡ç½®ï¼Œç»§ç»­çˆ¬å–")

    def perform_anti_ban_rest(self):
        """æ‰§è¡Œé˜²å°ç¦ä¼‘æ¯"""
        if not self.anti_ban_enabled:
            return

        config = self.anti_ban_config
        rest_time_min = config.get("rest_time_min", 600)
        
        # æ·»åŠ éšæœºæ³¢åŠ¨ï¼ˆÂ±10%ï¼‰
        rest_time = int(rest_time_min * random.uniform(0.9, 1.1))
        
        logger.info("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
        logger.info("â”‚ ğŸ›¡ï¸ é˜²å°ç¦ä¼‘æ¯ä¸­...                 â”‚")
        logger.info("â”‚ ä¼‘æ¯æ—¶é—´: %-4d ç§’                  â”‚", rest_time)
        logger.info("â”‚ é¢„è®¡æ¢å¤: %s       â”‚", 
                   (datetime.now() + timedelta(seconds=rest_time)).strftime("%H:%M:%S"))
        logger.info("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
        
        # æ‰§è¡Œä¼‘æ¯
        sleep(rest_time)
        
        logger.info("ä¼‘æ¯ç»“æŸï¼Œç»§ç»­çˆ¬å–å¾®åš")

    def validate_config(self, config):
        """éªŒè¯é…ç½®æ˜¯å¦æ­£ç¡®"""

        # éªŒè¯å¦‚ä¸‹1/0ç›¸å…³å€¼
        argument_list = [
            "only_crawl_original",
            "original_pic_download",
            "retweet_pic_download",
            "original_video_download",
            "retweet_video_download",
            "original_live_photo_download",
            "retweet_live_photo_download",
            "download_comment",
            "download_repost",
        ]
        for argument in argument_list:
            # ä½¿ç”¨ get() è·å–å€¼ï¼Œæ–°å¢å­—æ®µé»˜è®¤ä¸º0
            value = config.get(argument, 0)
            if value != 0 and value != 1:
                logger.warning("%så€¼åº”ä¸º0æˆ–1,è¯·é‡æ–°è¾“å…¥", argument)
                sys.exit()

        # éªŒè¯query_list
        query_list = config.get("query_list") or []
        if (not isinstance(query_list, list)) and (not isinstance(query_list, str)):
            logger.warning("query_listå€¼åº”ä¸ºlistç±»å‹æˆ–å­—ç¬¦ä¸²,è¯·é‡æ–°è¾“å…¥")
            sys.exit()

        # éªŒè¯write_mode
        write_mode = ["csv", "json", "mongo", "mysql", "sqlite", "post", "markdown"]
        if not isinstance(config["write_mode"], list):
            sys.exit("write_modeå€¼åº”ä¸ºlistç±»å‹")
        for mode in config["write_mode"]:
            if mode not in write_mode:
                logger.warning(
                    "%sä¸ºæ— æ•ˆæ¨¡å¼ï¼Œè¯·ä»csvã€jsonã€mongoã€mysqlã€sqliteã€postã€markdownä¸­æŒ‘é€‰ä¸€ä¸ªæˆ–å¤šä¸ªä½œä¸ºwrite_mode", mode
                )
                sys.exit()
        # éªŒè¯è¿è¡Œæ¨¡å¼
        if "sqlite" not in config["write_mode"] and const.MODE == "append":
            logger.warning("appendæ¨¡å¼ä¸‹è¯·å°†sqliteåŠ å…¥write_modeä¸­")
            sys.exit()

        # éªŒè¯user_id_list
        user_id_list = config["user_id_list"]
        if (not isinstance(user_id_list, list)) and (not user_id_list.endswith(".txt")):
            logger.warning("user_id_listå€¼åº”ä¸ºlistç±»å‹æˆ–txtæ–‡ä»¶è·¯å¾„")
            sys.exit()
        if not isinstance(user_id_list, list):
            if not os.path.isabs(user_id_list):
                user_id_list = (
                    os.path.split(os.path.realpath(__file__))[0] + os.sep + user_id_list
                )
            if not os.path.isfile(user_id_list):
                logger.warning("ä¸å­˜åœ¨%sæ–‡ä»¶", user_id_list)
                sys.exit()

        # éªŒè¯since_date
        since_date = config["since_date"]
        if (not isinstance(since_date, int)) and (not self.is_datetime(since_date)) and (not self.is_date(since_date)):
            logger.warning("since_dateå€¼åº”ä¸ºyyyy-mm-ddå½¢å¼ã€yyyy-mm-ddTHH:MM:SSå½¢å¼æˆ–æ•´æ•°ï¼Œè¯·é‡æ–°è¾“å…¥")
            sys.exit()

        comment_max_count = config["comment_max_download_count"]
        if not isinstance(comment_max_count, int):
            logger.warning("æœ€å¤§ä¸‹è½½è¯„è®ºæ•° (comment_max_download_count) åº”ä¸ºæ•´æ•°ç±»å‹")
            sys.exit()
        elif comment_max_count < 0:
            logger.warning("æœ€å¤§ä¸‹è½½è¯„è®ºæ•° (comment_max_download_count) åº”è¯¥ä¸ºæ­£æ•´æ•°")
            sys.exit()

        repost_max_count = config["repost_max_download_count"]
        if not isinstance(repost_max_count, int):
            logger.warning("æœ€å¤§ä¸‹è½½è½¬å‘æ•° (repost_max_download_count) åº”ä¸ºæ•´æ•°ç±»å‹")
            sys.exit()
        elif repost_max_count < 0:
            logger.warning("æœ€å¤§ä¸‹è½½è½¬å‘æ•° (repost_max_download_count) åº”è¯¥ä¸ºæ­£æ•´æ•°")
            sys.exit()

    def is_datetime(self, since_date):
        """åˆ¤æ–­æ—¥æœŸæ ¼å¼æ˜¯å¦ä¸º %Y-%m-%dT%H:%M:%S"""
        try:
            datetime.strptime(since_date, DTFORMAT)
            return True
        except ValueError:
            return False
    
    def is_date(self, since_date):
        """åˆ¤æ–­æ—¥æœŸæ ¼å¼æ˜¯å¦ä¸º %Y-%m-%d"""
        try:
            datetime.strptime(since_date, "%Y-%m-%d")
            return True
        except ValueError:
            return False

    def get_json(self, params):
        url = "https://m.weibo.cn/api/container/getIndex?"
        try:
            r = self.session.get(url, params=params, headers=self.headers, verify=False, timeout=10)
            r.raise_for_status()
            response_json = r.json()
            return response_json, r.status_code
        except RequestException as e:
            logger.error(f"è¯·æ±‚å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{e}")
            return {}, 500
        except ValueError as ve:
            logger.error(f"JSON è§£ç å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{ve}")
            return {}, 500

    def handle_captcha(self, js):
        """
        å¤„ç†éªŒè¯ç æŒ‘æˆ˜ï¼Œæç¤ºç”¨æˆ·æ‰‹åŠ¨å®ŒæˆéªŒè¯ã€‚

        å‚æ•°:
            js (dict): API è¿”å›çš„ JSON æ•°æ®ã€‚

        è¿”å›:
            bool: å¦‚æœç”¨æˆ·æˆåŠŸå®ŒæˆéªŒè¯ç ï¼Œè¿”å› Trueï¼›å¦åˆ™è¿”å› Falseã€‚
        """
        logger.debug(f"æ”¶åˆ°çš„ JSON æ•°æ®ï¼š{js}")
        
        captcha_url = js.get("url")
        if captcha_url:
            logger.warning("æ£€æµ‹åˆ°éªŒè¯ç æŒ‘æˆ˜ã€‚æ­£åœ¨æ‰“å¼€éªŒè¯ç é¡µé¢ä»¥ä¾›æ‰‹åŠ¨éªŒè¯ã€‚")
            webbrowser.open(captcha_url)
        else:
            logger.warning("æ£€æµ‹åˆ°å¯èƒ½çš„éªŒè¯ç æŒ‘æˆ˜ï¼Œä½†æœªæä¾›éªŒè¯ç  URLã€‚è¯·æ‰‹åŠ¨æ£€æŸ¥æµè§ˆå™¨å¹¶å®ŒæˆéªŒè¯ç éªŒè¯ã€‚")
            return False
        
        logger.info("è¯·åœ¨æ‰“å¼€çš„æµè§ˆå™¨çª—å£ä¸­å®ŒæˆéªŒè¯ç éªŒè¯ã€‚")
        while True:
            try:
                # ç­‰å¾…ç”¨æˆ·è¾“å…¥
                user_input = input("å®ŒæˆéªŒè¯ç åï¼Œè¯·è¾“å…¥ 'y' ç»§ç»­ï¼Œæˆ–è¾“å…¥ 'q' é€€å‡ºï¼š").strip().lower()

                if user_input == 'y':
                    logger.info("ç”¨æˆ·è¾“å…¥ 'y'ï¼Œç»§ç»­çˆ¬å–ã€‚")
                    return True
                elif user_input == 'q':
                    logger.warning("ç”¨æˆ·é€‰æ‹©é€€å‡ºï¼Œç¨‹åºä¸­æ­¢ã€‚")
                    sys.exit("ç”¨æˆ·é€‰æ‹©é€€å‡ºï¼Œç¨‹åºä¸­æ­¢ã€‚")
                else:
                    logger.warning("æ— æ•ˆè¾“å…¥ï¼Œè¯·é‡æ–°è¾“å…¥ 'y' æˆ– 'q'ã€‚")
            except EOFError:
                logger.error("è¯»å–ç”¨æˆ·è¾“å…¥æ—¶å‘ç”Ÿ EOFErrorï¼Œç¨‹åºé€€å‡ºã€‚")
                sys.exit("è¾“å…¥æµå·²å…³é—­ï¼Œç¨‹åºä¸­æ­¢ã€‚")
    
    def get_weibo_json(self, page):
        """è·å–ç½‘é¡µä¸­å¾®åšjsonæ•°æ®"""
        url = "https://m.weibo.cn/api/container/getIndex?"
        params = (
            {
                "container_ext": "profile_uid:" + str(self.user_config["user_id"]),
                "containerid": "100103type=401&q=" + self.query,
                "page_type": "searchall",
            }
            if self.query
            else {"containerid": "230413" + str(self.user_config["user_id"])}
        )
        params["page"] = page
        params["count"] = self.page_weibo_count
        max_retries = 5
        retries = 0
        backoff_factor = 5

        while retries < max_retries:
            try:
                # é˜²å°ç¦ï¼šä½¿ç”¨éšæœºè¯·æ±‚å¤´
                current_headers = self.get_random_headers()

                # é˜²å°ç¦ï¼šåŠ¨æ€å»¶è¿Ÿ
                delay = self.calculate_dynamic_delay()
                if delay > 0:
                    logger.debug(f"åŠ¨æ€å»¶è¿Ÿ: {delay:.1f} ç§’")
                    sleep(delay)

                response = self.session.get(url, params=params, headers=current_headers, timeout=10)
                response.raise_for_status()  # å¦‚æœå“åº”çŠ¶æ€ç ä¸æ˜¯ 200ï¼Œä¼šæŠ›å‡º HTTPError
                js = response.json()

                # æ›´æ–°ç»Ÿè®¡ï¼šæˆåŠŸè¯·æ±‚
                self.update_crawl_stats(request_count=1)

                if 'data' in js:
                    logger.info(f"æˆåŠŸè·å–åˆ°é¡µé¢ {page} çš„æ•°æ®ã€‚")
                    return js
                else:
                    logger.warning("æœªèƒ½è·å–åˆ°æ•°æ®ï¼Œå¯èƒ½éœ€è¦éªŒè¯ç éªŒè¯ã€‚")
                    if self.handle_captcha(js):
                        logger.info("ç”¨æˆ·å·²å®ŒæˆéªŒè¯ç éªŒè¯ï¼Œç»§ç»­è¯·æ±‚æ•°æ®ã€‚")
                        retries = 0  # é‡ç½®é‡è¯•è®¡æ•°å™¨
                        continue
                    else:
                        logger.error("éªŒè¯ç éªŒè¯å¤±è´¥æˆ–æœªå®Œæˆï¼Œç¨‹åºå°†é€€å‡ºã€‚")
                        sys.exit()
            except RequestException as e:
                retries += 1
                sleep_time = backoff_factor * (2 ** retries)
                logger.error(f"è¯·æ±‚å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{e}ã€‚ç­‰å¾… {sleep_time} ç§’åé‡è¯•...")
                sleep(sleep_time)
                # æ›´æ–°ç»Ÿè®¡ï¼šAPIé”™è¯¯
                self.update_crawl_stats(api_error=True)
            except ValueError as ve:
                retries += 1
                sleep_time = backoff_factor * (2 ** retries)
                logger.error(f"JSON è§£ç å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{ve}ã€‚ç­‰å¾… {sleep_time} ç§’åé‡è¯•...")
                sleep(sleep_time)
                # æ›´æ–°ç»Ÿè®¡ï¼šAPIé”™è¯¯
                self.update_crawl_stats(api_error=True)

        logger.error("è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè·³è¿‡å½“å‰é¡µé¢ã€‚")
        return {}
    
    def user_to_csv(self):
        """å°†çˆ¬å–åˆ°çš„ç”¨æˆ·ä¿¡æ¯å†™å…¥csvæ–‡ä»¶"""
        file_dir = os.path.split(os.path.realpath(__file__))[0] + os.sep + "weibo"
        if not os.path.isdir(file_dir):
            os.makedirs(file_dir)
        file_path = file_dir + os.sep + "users.csv"
        self.user_csv_file_path = file_path
        result_headers = [
            "ç”¨æˆ·id",
            "æ˜µç§°",
            "æ€§åˆ«",
            "ç”Ÿæ—¥",
            "æ‰€åœ¨åœ°",
            "IPå±åœ°",
            "å­¦ä¹ ç»å†",
            "å…¬å¸",
            "æ³¨å†Œæ—¶é—´",
            "é˜³å…‰ä¿¡ç”¨",
            "å¾®åšæ•°",
            "ç²‰ä¸æ•°",
            "å…³æ³¨æ•°",
            "ç®€ä»‹",
            "ä¸»é¡µ",
            "å¤´åƒ",
            "é«˜æ¸…å¤´åƒ",
            "å¾®åšç­‰çº§",
            "ä¼šå‘˜ç­‰çº§",
            "æ˜¯å¦è®¤è¯",
            "è®¤è¯ç±»å‹",
            "è®¤è¯ä¿¡æ¯",
            "ä¸Šæ¬¡è®°å½•å¾®åšä¿¡æ¯",
        ]
        result_data = [
            [
                v.encode("utf-8") if "unicode" in str(type(v)) else v
                for v in self.user.values()
            ]
        ]
        # å·²ç»æ’å…¥ä¿¡æ¯çš„ç”¨æˆ·æ— éœ€é‡å¤æ’å…¥ï¼Œè¿”å›çš„idæ˜¯ç©ºå­—ç¬¦ä¸²æˆ–å¾®åšid å‘å¸ƒæ—¥æœŸ%Y-%m-%d
        last_weibo_msg = csvutil.insert_or_update_user(
            logger, result_headers, result_data, file_path
        )
        self.last_weibo_id = last_weibo_msg.split(" ")[0] if last_weibo_msg else ""
        self.last_weibo_date = (
            last_weibo_msg.split(" ")[1]
            if last_weibo_msg
            else self.user_config["since_date"]
        )

    def user_to_mongodb(self):
        """å°†çˆ¬å–çš„ç”¨æˆ·ä¿¡æ¯å†™å…¥MongoDBæ•°æ®åº“"""
        user_list = [self.user]
        self.info_to_mongodb("user", user_list)
        logger.info("%sä¿¡æ¯å†™å…¥MongoDBæ•°æ®åº“å®Œæ¯•", self.user["screen_name"])

    def user_to_mysql(self):
        """å°†çˆ¬å–çš„ç”¨æˆ·ä¿¡æ¯å†™å…¥MySQLæ•°æ®åº“"""
        mysql_config = {
            "host": "localhost",
            "port": 3306,
            "user": "root",
            "password": "123456",
            "charset": "utf8mb4",
        }
        # åˆ›å»º'weibo'æ•°æ®åº“
        create_database = """CREATE DATABASE IF NOT EXISTS weibo DEFAULT
                         CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci"""
        self.mysql_create_database(mysql_config, create_database)
        # åˆ›å»º'user'è¡¨
        create_table = """
                CREATE TABLE IF NOT EXISTS user (
                id varchar(20) NOT NULL,
                screen_name varchar(30),
                gender varchar(10),
                statuses_count INT,
                followers_count INT,
                follow_count INT,
                registration_time varchar(20),
                sunshine varchar(20),
                birthday varchar(40),
                location varchar(200),
                ip_location varchar(50),
                education varchar(200),
                company varchar(200),
                description varchar(400),
                profile_url varchar(200),
                profile_image_url varchar(200),
                avatar_hd varchar(200),
                urank INT,
                mbrank INT,
                verified BOOLEAN DEFAULT 0,
                verified_type INT,
                verified_reason varchar(140),
                PRIMARY KEY (id)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4"""
        self.mysql_create_table(mysql_config, create_table)
        self.mysql_insert(mysql_config, "user", [self.user])
        logger.info("%sä¿¡æ¯å†™å…¥MySQLæ•°æ®åº“å®Œæ¯•", self.user["screen_name"])

    def user_to_database(self):
        """å°†ç”¨æˆ·ä¿¡æ¯å†™å…¥æ–‡ä»¶/æ•°æ®åº“"""
        self.user_to_csv()
        if "mysql" in self.write_mode:
            self.user_to_mysql()
        if "mongo" in self.write_mode:
            self.user_to_mongodb()
        if "sqlite" in self.write_mode:
            self.user_to_sqlite()

    def get_user_info(self):
        """è·å–ç”¨æˆ·ä¿¡æ¯"""
        params = {"containerid": "100505" + str(self.user_config["user_id"])}
        url = "https://m.weibo.cn/api/container/getIndex"
        
        # è¿™é‡Œåœ¨è¯»å–ä¸‹ä¸€ä¸ªç”¨æˆ·çš„æ—¶å€™å¾ˆå®¹æ˜“è¢«banï¼Œéœ€è¦ä¼˜åŒ–ä¼‘çœ æ—¶é•¿
        # åŠ ä¸€ä¸ªcountï¼Œä¸éœ€è¦ä¸€ä¸Šæ¥å•¥éƒ½æ²¡å¹²å°±sleep
        if self.long_sleep_count_before_each_user > 0:
            sleep_time = random.randint(30, 60)
            # æ·»åŠ logï¼Œå¦åˆ™ä¸€èˆ¬ç”¨æˆ·ä¸çŸ¥é“ä»¥ä¸ºç¨‹åºå¡äº†
            logger.info(f"""çŸ­æš‚sleep {sleep_time}ç§’ï¼Œé¿å…è¢«ban""")        
            sleep(sleep_time)
            logger.info("sleepç»“æŸ")  
        self.long_sleep_count_before_each_user = self.long_sleep_count_before_each_user + 1      

        max_retries = 5  # è®¾ç½®æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé¿å…æ— é™å¾ªç¯
        retries = 0
        backoff_factor = 5  # æŒ‡æ•°é€€é¿çš„åŸºæ•°ï¼ˆç§’ï¼‰
        
        while retries < max_retries:
            try:
                logger.info(f"å‡†å¤‡è·å–IDï¼š{self.user_config['user_id']}çš„ç”¨æˆ·ä¿¡æ¯ç¬¬{retries+1}æ¬¡ã€‚")

                # é˜²å°ç¦ï¼šä½¿ç”¨éšæœºè¯·æ±‚å¤´
                current_headers = self.get_random_headers()

                # é˜²å°ç¦ï¼šåŠ¨æ€å»¶è¿Ÿ
                delay = self.calculate_dynamic_delay()
                if delay > 0:
                    logger.debug(f"åŠ¨æ€å»¶è¿Ÿ: {delay:.1f} ç§’")
                    sleep(delay)

                response = self.session.get(url, params=params, headers=current_headers, timeout=10)
                response.raise_for_status()
                js = response.json()

                # æ›´æ–°ç»Ÿè®¡ï¼šæˆåŠŸè¯·æ±‚
                self.update_crawl_stats(request_count=1)
                if 'data' in js and 'userInfo' in js['data']:
                    info = js["data"]["userInfo"]
                    user_info = OrderedDict()
                    user_info["id"] = self.user_config["user_id"]
                    user_info["screen_name"] = info.get("screen_name", "")
                    user_info["gender"] = info.get("gender", "")
                    params = {
                        "containerid": "230283" + str(self.user_config["user_id"]) + "_-_INFO"
                    }
                    zh_list = ["ç”Ÿæ—¥", "æ‰€åœ¨åœ°", "IPå±åœ°", "å°å­¦", "åˆä¸­", "é«˜ä¸­", "å¤§å­¦", "å…¬å¸", "æ³¨å†Œæ—¶é—´", "é˜³å…‰ä¿¡ç”¨"]
                    en_list = [
                        "birthday",
                        "location",
                        "ip_location",
                        "education",
                        "education",
                        "education",
                        "education",
                        "company",
                        "registration_time",
                        "sunshine",
                    ]
                    for i in en_list:
                        user_info[i] = ""
                    js, _ = self.get_json(params)
                    if js["ok"]:
                        cards = js["data"]["cards"]
                        if isinstance(cards, list) and len(cards) > 1:
                            card_list = cards[0]["card_group"] + cards[1]["card_group"]
                            for card in card_list:
                                if card.get("item_name") in zh_list:
                                    user_info[
                                        en_list[zh_list.index(card.get("item_name"))]
                                    ] = card.get("item_content", "")
                    user_info["statuses_count"] = self.string_to_int(
                        info.get("statuses_count", 0)
                    )
                    user_info["followers_count"] = self.string_to_int(
                        info.get("followers_count", 0)
                    )
                    user_info["follow_count"] = self.string_to_int(info.get("follow_count", 0))
                    user_info["description"] = info.get("description", "")
                    user_info["profile_url"] = info.get("profile_url", "")
                    user_info["profile_image_url"] = info.get("profile_image_url", "")
                    user_info["avatar_hd"] = info.get("avatar_hd", "")
                    user_info["urank"] = info.get("urank", 0)
                    user_info["mbrank"] = info.get("mbrank", 0)
                    user_info["verified"] = info.get("verified", False)
                    user_info["verified_type"] = info.get("verified_type", -1)
                    user_info["verified_reason"] = info.get("verified_reason", "")
                    self.user = self.standardize_info(user_info)
                    self.user_to_database()
                    logger.info(f"æˆåŠŸè·å–åˆ°ç”¨æˆ· {self.user_config['user_id']} çš„ä¿¡æ¯ã€‚")
                    return 0
                elif isinstance(js.get("url"), str) and js.get("url").strip():
                    logger.warning("æœªèƒ½è·å–åˆ°ç”¨æˆ·ä¿¡æ¯ï¼Œå¯èƒ½éœ€è¦éªŒè¯ç éªŒè¯ã€‚")
                    if self.handle_captcha(js):
                        logger.info("ç”¨æˆ·å·²å®ŒæˆéªŒè¯ç éªŒè¯ï¼Œç»§ç»­è¯·æ±‚ç”¨æˆ·ä¿¡æ¯ã€‚")
                        retries = 0  # é‡ç½®é‡è¯•è®¡æ•°å™¨
                        continue
                    else:
                        logger.error("éªŒè¯ç éªŒè¯å¤±è´¥æˆ–æœªå®Œæˆï¼Œç¨‹åºå°†é€€å‡ºã€‚")
                        sys.exit()
                elif isinstance(js.get("msg"), str) and "è¿™é‡Œè¿˜æ²¡æœ‰å†…å®¹" in js.get("msg"):
                    logger.warning("æœªèƒ½è·å–åˆ°ç”¨æˆ·ä¿¡æ¯ï¼Œå¯èƒ½è´¦å·å·²æ³¨é”€æˆ–ç”¨æˆ·idæœ‰è¯¯ã€‚")
                    return 1
                else:
                    logger.warning("æœªèƒ½è·å–åˆ°ç”¨æˆ·ä¿¡æ¯ã€‚")
                    return 1
            except RequestException as e:
                retries += 1
                sleep_time = backoff_factor * (2 ** retries)
                logger.error(f"è¯·æ±‚å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{e}ã€‚ç­‰å¾… {sleep_time} ç§’åé‡è¯•...")
                sleep(sleep_time)
                # æ›´æ–°ç»Ÿè®¡ï¼šAPIé”™è¯¯
                self.update_crawl_stats(api_error=True)
            except ValueError as ve:
                retries += 1
                sleep_time = backoff_factor * (2 ** retries)
                logger.error(f"JSON è§£ç å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{ve}ã€‚ç­‰å¾… {sleep_time} ç§’åé‡è¯•...")
                sleep(sleep_time)
                # æ›´æ–°ç»Ÿè®¡ï¼šAPIé”™è¯¯
                self.update_crawl_stats(api_error=True)
        logger.error("è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œç¨‹åºå°†é€€å‡ºã€‚")
        sys.exit("è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œç¨‹åºå·²é€€å‡ºã€‚")

    def get_long_weibo(self, id):
        """è·å–é•¿å¾®åš"""
        url = "https://m.weibo.cn/detail/%s" % id
        logger.info(f"""URL: {url} """)
        for i in range(5):
            sleep(random.uniform(1.0, 2.5))
            html = self.session.get(url, headers=self.headers, verify=False).text
            html = html[html.find('"status":') :]
            html = html[: html.rfind('"call"')]
            html = html[: html.rfind(",")]
            html = "{" + html + "}"
            js = json.loads(html, strict=False)
            weibo_info = js.get("status")
            if weibo_info:
                weibo = self.parse_weibo(weibo_info)
                return weibo

    def get_pics(self, weibo_info):
        """è·å–å¾®åšåŸå§‹å›¾ç‰‡url"""
        if weibo_info.get("pics"):
            pic_info = weibo_info["pics"]
            pic_list = [
                pic['large']['url'] for pic in pic_info
                if isinstance(pic, dict) and pic.get('large')
            ]
            pics = ",".join(pic_list)
        else:
            pics = ""
        return pics


    def get_live_photo_url(self, weibo_info):
        """è·å–Live Photoè§†é¢‘URL"""
        live_photo_list = weibo_info.get("live_photo", [])
        return ";".join(live_photo_list) if live_photo_list else ""
    def get_video_url(self, weibo_info):
        """è·å–å¾®åšæ™®é€šè§†é¢‘URL"""
        video_url = ""
        if weibo_info.get("page_info"):
            if weibo_info["page_info"].get("type") == "video":
                media_info = weibo_info["page_info"].get("urls") or weibo_info["page_info"].get("media_info")
                if media_info:
                    video_url = (media_info.get("mp4_720p_mp4") or
                                media_info.get("mp4_hd_url") or
                                media_info.get("hevc_mp4_hd") or
                                media_info.get("mp4_sd_url") or
                                media_info.get("mp4_ld_mp4") or
                                media_info.get("stream_url_hd") or
                                media_info.get("stream_url"))
        return video_url

    def write_exif_time(self, file_path, time_str):
        if self.write_time_in_exif:
            """å†™å…¥ JPG EXIF å…ƒæ•°æ®"""
            try:
                # å°† "2025-09-06T22:16:36" è½¬æ¢ä¸º "2025:09:06 22:16:36"
                exif_time = time_str.replace("-", ":").replace("T", " ")[:19]
                exif_dict = {"Exif": {piexif.ExifIFD.DateTimeOriginal: exif_time}}
                exif_bytes = piexif.dump(exif_dict)
                piexif.insert(exif_bytes, file_path)
                logger.debug(f"[EXIF] å·²å°†æ—¶é—´ {exif_time} å†™å…¥ {file_path}")
            except Exception as e:
                logger.debug(f"EXIFå†™å…¥è·³è¿‡æˆ–å¤±è´¥: {e}")

    def set_file_time(self, file_path, time_str):
        if self.change_file_time:
            """ä¿®æ”¹æ–‡ä»¶ç³»ç»Ÿæ—¶é—´ï¼ˆä¿®æ”¹æ—¥æœŸï¼‰"""
            try:
                # å…¼å®¹å¸¦ T æˆ–ä¸å¸¦ T çš„æ ¼å¼
                clean_time = time_str.replace("T", " ")
                tick = time.mktime(time.strptime(clean_time, "%Y-%m-%d %H:%M:%S"))
                # åŒæ—¶ä¿®æ”¹è®¿é—®æ—¶é—´å’Œä¿®æ”¹æ—¶é—´
                os.utime(file_path, (tick, tick))
                logger.debug(f"[FILE] å·²å°†æ—¶é—´ {clean_time} å†™å…¥ {file_path}")
            except Exception as e:
                logger.debug(f"ä¿®æ”¹æ–‡ä»¶ç³»ç»Ÿæ—¶é—´å¤±è´¥: {e}")

    def download_one_file(self, url, file_path, type, weibo_id, created_at):
        """ä¸‹è½½å•ä¸ªæ–‡ä»¶(å›¾ç‰‡/è§†é¢‘)"""
        try:

            file_exist = os.path.isfile(file_path)
            need_download = (not file_exist)
            sqlite_exist = False
            if "sqlite" in self.write_mode:
                sqlite_exist = self.sqlite_exist_file(file_path)

            if not need_download:
                return 

            s = requests.Session()
            s.mount('http://', HTTPAdapter(max_retries=5))
            s.mount('https://', HTTPAdapter(max_retries=5))
            try_count = 0
            success = False
            MAX_TRY_COUNT = 3
            detected_extension = None
            while try_count < MAX_TRY_COUNT:
                try:
                    response = s.get(
                        url, headers=self.headers, timeout=(5, 10), verify=False
                    )
                    response.raise_for_status()
                    downloaded = response.content
                    try_count += 1

                    # è·å–æ–‡ä»¶åç¼€
                    url_path = url.split('?')[0]  # å»é™¤URLä¸­çš„å‚æ•°
                    inferred_extension = os.path.splitext(url_path)[1].lower().strip('.')

                    # é€šè¿‡ Magic Number æ£€æµ‹æ–‡ä»¶ç±»å‹
                    if downloaded.startswith(b'\xFF\xD8\xFF'):
                        # JPEG æ–‡ä»¶
                        if not downloaded.endswith(b'\xff\xd9'):
                            logger.debug(f"[DEBUG] JPEG æ–‡ä»¶ä¸å®Œæ•´: {url} ({try_count}/{MAX_TRY_COUNT})")
                            continue  # æ–‡ä»¶ä¸å®Œæ•´ï¼Œç»§ç»­é‡è¯•
                        detected_extension = '.jpg'
                    elif downloaded.startswith(b'\x89PNG\r\n\x1A\n'):
                        # PNG æ–‡ä»¶
                        if not downloaded.endswith(b'IEND\xaeB`\x82'):
                            logger.debug(f"[DEBUG] PNG æ–‡ä»¶ä¸å®Œæ•´: {url} ({try_count}/{MAX_TRY_COUNT})")
                            continue  # æ–‡ä»¶ä¸å®Œæ•´ï¼Œç»§ç»­é‡è¯•
                        detected_extension = '.png'
                    else:
                        # å…¶ä»–ç±»å‹ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘å¤„ç†
                        if inferred_extension in ['mp4', 'mov', 'webm', 'gif', 'bmp', 'tiff']:
                            detected_extension = '.' + inferred_extension
                        else:
                            # å°è¯•ä» Content-Type è·å–æ‰©å±•å
                            content_type = response.headers.get('Content-Type', '').lower()
                            if 'image/jpeg' in content_type:
                                detected_extension = '.jpg'
                            elif 'image/png' in content_type:
                                detected_extension = '.png'
                            elif 'video/mp4' in content_type:
                                detected_extension = '.mp4'
                            elif 'video/quicktime' in content_type:
                                detected_extension = '.mov'
                            elif 'video/webm' in content_type:
                                detected_extension = '.webm'
                            elif 'image/gif' in content_type:
                                detected_extension = '.gif'
                            else:
                                # ä½¿ç”¨åŸæœ‰çš„æ‰©å±•åï¼Œå¦‚æœæ— æ³•ç¡®å®š
                                detected_extension = '.' + inferred_extension if inferred_extension else ''

                    # åŠ¨æ€è°ƒæ•´æ–‡ä»¶è·¯å¾„çš„æ‰©å±•å
                    if detected_extension:
                        file_path = re.sub(r'\.\w+$', detected_extension, file_path)

                    # ä¿å­˜æ–‡ä»¶
                    if not os.path.isfile(file_path):
                        with open(file_path, "wb") as f:
                            f.write(downloaded)
                            logger.debug("[DEBUG] save " + file_path)
                        if detected_extension in ['.jpg', '.jpeg']:
                            try:
                                self.write_exif_time(file_path, created_at)
                            except Exception as e:
                                logger.error(f"å†™å…¥EXIFå¤±è´¥: {e}")
                        try:
                            # 1. æ— è®ºä»€ä¹ˆæ ¼å¼ï¼Œéƒ½ä¿®æ”¹ç³»ç»Ÿæ—¶é—´ (æ–¹ä¾¿æ–‡ä»¶å¤¹æ’åº)
                            self.set_file_time(file_path, created_at)
                        except Exception as e:
                            logger.error(f"ä¿®æ”¹æ–‡ä»¶ç³»ç»Ÿæ—¶é—´å¤±è´¥: {e}")

                    success = True
                    logger.debug("[DEBUG] success " + url + "  " + str(try_count))
                    break  # ä¸‹è½½æˆåŠŸï¼Œé€€å‡ºé‡è¯•å¾ªç¯

                except RequestException as e:
                    try_count += 1
                    logger.error(f"[ERROR] è¯·æ±‚å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{e}ã€‚å°è¯•æ¬¡æ•°ï¼š{try_count}/{MAX_TRY_COUNT}")
                    sleep_time = 2 ** try_count  # æŒ‡æ•°é€€é¿
                    sleep(sleep_time)
                except Exception as e:
                    logger.exception(f"[ERROR] ä¸‹è½½è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                    break  # å¯¹äºå…¶ä»–å¼‚å¸¸ï¼Œé€€å‡ºé‡è¯•

            if success:
                if "sqlite" in self.write_mode and not sqlite_exist:
                    self.insert_file_sqlite(
                        file_path, weibo_id, url, downloaded
                    )
            else:
                logger.debug("[DEBUG] failed " + url + " TOTALLY")
                error_file = self.get_filepath(type) + os.sep + "not_downloaded.txt"
                with open(error_file, "ab") as f:
                    error_entry = f"{weibo_id}:{file_path}:{url}\n"
                    f.write(error_entry.encode(sys.stdout.encoding))
        except Exception as e:
            # ç”ŸæˆåŸå§‹å¾®åšURL
            original_url = f"https://m.weibo.cn/detail/{weibo_id}"  # æ–°å¢
            error_file = self.get_filepath(type) + os.sep + "not_downloaded.txt"
            with open(error_file, "ab") as f:
                # ä¿®æ”¹é”™è¯¯æ¡ç›®æ ¼å¼ï¼Œæ·»åŠ åŸå§‹URL
                error_entry = f"{weibo_id}:{file_path}:{url}:{original_url}\n"  # ä¿®æ”¹
                f.write(error_entry.encode(sys.stdout.encoding))
            logger.exception(e)

    def sqlite_exist_file(self, url):
        if not os.path.exists(self.get_sqlte_path()):
            return True
        con = self.get_sqlite_connection()
        cur = con.cursor()

        query_sql = """SELECT url FROM bins WHERE path=? """
        count = cur.execute(query_sql, (url,)).fetchone()
        con.close()
        if count is None:
            return False

        return True

    def insert_file_sqlite(self, file_path, weibo_id, url, binary):
        if not weibo_id:
            return
        if self.store_binary_in_sqlite != 1:  # æ–°å¢é…ç½®åˆ¤æ–­
            return
        extension = Path(file_path).suffix
        if not extension:
            return
        if len(binary) <= 0:
            return

        file_data = OrderedDict()
        file_data["weibo_id"] = weibo_id
        file_data["ext"] = extension
        file_data["data"] = binary  # ä»…å½“å¯ç”¨æ—¶å­˜å‚¨äºŒè¿›åˆ¶
        file_data["path"] = file_path
        file_data["url"] = url

        con = self.get_sqlite_connection()
        self.sqlite_insert(con, file_data, "bins")
        con.close()

    def handle_download(self, file_type, file_dir, urls, w):
        """å¤„ç†ä¸‹è½½ç›¸å…³æ“ä½œ"""
        file_prefix = w["created_at"][:11].replace("-", "") + "_" + str(w["id"])
        if file_type == "img":
            if "," in urls:
                url_list = urls.split(",")
                for i, url in enumerate(url_list):
                    index = url.rfind(".")
                    if len(url) - index >= 5:
                        file_suffix = ".jpg"
                    else:
                        file_suffix = url[index:]
                    file_name = file_prefix + "_" + str(i + 1) + file_suffix
                    file_path = file_dir + os.sep + file_name
                    self.download_one_file(url, file_path, file_type, w["id"], w["created_at"])
            else:
                index = urls.rfind(".")
                if len(urls) - index > 5:
                    file_suffix = ".jpg"
                else:
                    file_suffix = urls[index:]
                file_name = file_prefix + file_suffix
                file_path = file_dir + os.sep + file_name
                self.download_one_file(urls, file_path, file_type, w["id"], w["created_at"])
        elif file_type == "video" or file_type == "live_photo":
            file_suffix = ".mp4"
            if ";" in urls:
                url_list = urls.split(";")
                for i, url in enumerate(url_list):
                    if url.endswith(".mov"):
                        file_suffix = ".mov"
                    file_name = file_prefix + "_" + str(i + 1) + file_suffix
                    file_path = file_dir + os.sep + file_name
                    self.download_one_file(url, file_path, file_type, w["id"], w["created_at"])
            else:
                if urls.endswith(".mov"):
                    file_suffix = ".mov"
                file_name = file_prefix + file_suffix
                file_path = file_dir + os.sep + file_name
                self.download_one_file(urls, file_path, file_type, w["id"], w["created_at"])

    def download_files(self, file_type, weibo_type, wrote_count):
        try:
            describe = ""
            if file_type == "img":
                describe = "å›¾ç‰‡"
                key = "pics"
            elif file_type == "video":
                describe = "è§†é¢‘"
                key = "video_url"
            elif file_type == "live_photo":
                describe = "Live Photoè§†é¢‘"
                key = "live_photo_url"
            else:
                return
            
            if weibo_type == "original":
                describe = "åŸåˆ›å¾®åš" + describe
            else:
                describe = "è½¬å‘å¾®åš" + describe
            
            logger.info("å³å°†è¿›è¡Œ%sä¸‹è½½", describe)
            file_dir = self.get_filepath(file_type)
            file_dir = file_dir + os.sep + describe
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶éœ€è¦ä¸‹è½½
            has_files = False
            for w in self.weibo[wrote_count:]:
                if weibo_type == "retweet":
                    if w.get("retweet"):
                        w = w["retweet"]
                    else:
                        continue
                if w.get(key):
                    has_files = True
                    break
            
            if has_files:
                if not os.path.isdir(file_dir):
                    os.makedirs(file_dir)
                
                for w in tqdm(self.weibo[wrote_count:], desc="Download progress"):
                    if weibo_type == "retweet":
                        if w.get("retweet"):
                            w = w["retweet"]
                        else:
                            continue
                    if w.get(key):
                        self.handle_download(file_type, file_dir, w.get(key), w)
                
                logger.info("%sä¸‹è½½å®Œæ¯•,ä¿å­˜è·¯å¾„:", describe)
                logger.info(file_dir)
            else:
                logger.info("æ²¡æœ‰%séœ€è¦ä¸‹è½½", describe)
        except Exception as e:
            logger.exception(e)

    def get_location(self, selector):
        """è·å–å¾®åšå‘å¸ƒä½ç½®"""
        location_icon = "timeline_card_small_location_default.png"
        span_list = selector.xpath("//span")
        location = ""
        for i, span in enumerate(span_list):
            if span.xpath("img/@src"):
                if location_icon in span.xpath("img/@src")[0]:
                    location = span_list[i + 1].xpath("string(.)")
                    break
        return location

    def get_article_url(self, selector):
        """è·å–å¾®åšä¸­å¤´æ¡æ–‡ç« çš„url"""
        article_url = ""
        text = selector.xpath("string(.)")
        if text.startswith("å‘å¸ƒäº†å¤´æ¡æ–‡ç« "):
            url = selector.xpath("//a/@data-url")
            if url and url[0].startswith("http://t.cn"):
                article_url = url[0]
        return article_url

    def get_topics(self, selector):
        """è·å–å‚ä¸çš„å¾®åšè¯é¢˜"""
        span_list = selector.xpath("//span[@class='surl-text']")
        topics = ""
        topic_list = []
        for span in span_list:
            text = span.xpath("string(.)")
            if len(text) > 2 and text[0] == "#" and text[-1] == "#":
                topic_list.append(text[1:-1])
        if topic_list:
            topics = ",".join(topic_list)
        return topics

    def get_at_users(self, selector):
        """è·å–@ç”¨æˆ·"""
        a_list = selector.xpath("//a")
        at_users = ""
        at_list = []
        for a in a_list:
            if "@" + a.xpath("@href")[0][3:] == a.xpath("string(.)"):
                at_list.append(a.xpath("string(.)")[1:])
        if at_list:
            at_users = ",".join(at_list)
        return at_users

    def string_to_int(self, string):
        """å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•´æ•°"""
        if isinstance(string, int):
            return string
        elif string.endswith("ä¸‡+"):
            string = string[:-2] + "0000"
        elif string.endswith("ä¸‡"):
            string = float(string[:-1]) * 10000
        elif string.endswith("äº¿"):
            string = float(string[:-1]) * 100000000
        return int(string)

    def standardize_date(self, created_at):
        """æ ‡å‡†åŒ–å¾®åšå‘å¸ƒæ—¶é—´"""
        if "åˆšåˆš" in created_at:
            ts = datetime.now()
        elif "åˆ†é’Ÿ" in created_at:
            minute = created_at[: created_at.find("åˆ†é’Ÿ")]
            minute = timedelta(minutes=int(minute))
            ts = datetime.now() - minute
        elif "å°æ—¶" in created_at:
            hour = created_at[: created_at.find("å°æ—¶")]
            hour = timedelta(hours=int(hour))
            ts = datetime.now() - hour
        elif "æ˜¨å¤©" in created_at:
            day = timedelta(days=1)
            ts = datetime.now() - day
        else:
            created_at = created_at.replace("+0800 ", "")
            ts = datetime.strptime(created_at, "%c")

        created_at = ts.strftime(DTFORMAT)
        full_created_at = ts.strftime("%Y-%m-%d %H:%M:%S")
        return created_at, full_created_at

    def standardize_info(self, weibo):
        """æ ‡å‡†åŒ–ä¿¡æ¯ï¼Œå»é™¤ä¹±ç """
        for k, v in weibo.items():
            if (
                "bool" not in str(type(v))
                and "int" not in str(type(v))
                and "list" not in str(type(v))
                and "long" not in str(type(v))
            ):
                weibo[k] = (
                    v.replace("\u200b", "")
                    .encode(sys.stdout.encoding, "ignore")
                    .decode(sys.stdout.encoding)
                )
        return weibo

    def parse_weibo(self, weibo_info):
        weibo = OrderedDict()
        if weibo_info["user"]:
            weibo["user_id"] = weibo_info["user"]["id"]
            weibo["screen_name"] = weibo_info["user"]["screen_name"]
        else:
            weibo["user_id"] = ""
            weibo["screen_name"] = ""
        weibo["id"] = int(weibo_info["id"])
        weibo["bid"] = weibo_info["bid"]
        text_body = weibo_info["text"]
        selector = etree.HTML(f"{text_body}<hr>" if text_body.isspace() else text_body)
        if self.remove_html_tag:
            text_list = selector.xpath("//text()")
            # è‹¥text_listä¸­çš„æŸä¸ªå­—ç¬¦ä¸²å…ƒç´ ä»¥ @ æˆ– # å¼€å§‹ï¼Œåˆ™å°†è¯¥å…ƒç´ ä¸å‰åå…ƒç´ åˆå¹¶ä¸ºæ–°å…ƒç´ ï¼Œå¦åˆ™ä¼šå¸¦æ¥æ²¡æœ‰å¿…è¦çš„æ¢è¡Œ
            text_list_modified = []
            for ele in range(len(text_list)):
                if ele > 0 and (text_list[ele-1].startswith(('@','#')) or text_list[ele].startswith(('@','#'))):
                    text_list_modified[-1] += text_list[ele]
                else:
                    text_list_modified.append(text_list[ele])
            weibo["text"] = "\n".join(text_list_modified)
        else:
            weibo["text"] = text_body
        weibo["article_url"] = self.get_article_url(selector)
        weibo["pics"] = self.get_pics(weibo_info)
        weibo["video_url"] = self.get_video_url(weibo_info)  # æ™®é€šè§†é¢‘URL
        weibo["live_photo_url"] = self.get_live_photo_url(weibo_info)  # Live Photoè§†é¢‘URL
        weibo["location"] = self.get_location(selector)
        weibo["created_at"] = weibo_info["created_at"]
        weibo["source"] = weibo_info["source"]
        weibo["attitudes_count"] = self.string_to_int(
            weibo_info.get("attitudes_count", 0)
        )
        weibo["comments_count"] = self.string_to_int(
            weibo_info.get("comments_count", 0)
        )
        weibo["reposts_count"] = self.string_to_int(weibo_info.get("reposts_count", 0))
        weibo["topics"] = self.get_topics(selector)
        weibo["at_users"] = self.get_at_users(selector)
        
        # ä½¿ç”¨ LLM åˆ†æå¾®åšå†…å®¹
        if self.llm_analyzer:
            weibo = self.llm_analyzer.analyze_weibo(weibo)
            logger.info("å®Œæ•´åˆ†æç»“æœï¼š\n%s", json.dumps(weibo, ensure_ascii=False, indent=2))
        return self.standardize_info(weibo)

    def print_user_info(self):
        """æ‰“å°ç”¨æˆ·ä¿¡æ¯"""
        logger.info("+" * 100)
        logger.info("ç”¨æˆ·ä¿¡æ¯")
        logger.info("ç”¨æˆ·idï¼š%s", self.user["id"])
        logger.info("ç”¨æˆ·æ˜µç§°ï¼š%s", self.user["screen_name"])
        gender = "å¥³" if self.user["gender"] == "f" else "ç”·"
        logger.info("æ€§åˆ«ï¼š%s", gender)
        logger.info("ç”Ÿæ—¥ï¼š%s", self.user["birthday"])
        logger.info("æ‰€åœ¨åœ°ï¼š%s", self.user["location"])
        logger.info("IPå±åœ°ï¼š%s", self.user.get("ip_location", "æœªè·å–"))        
        logger.info("æ•™è‚²ç»å†ï¼š%s", self.user["education"])
        logger.info("å…¬å¸ï¼š%s", self.user["company"])
        logger.info("é˜³å…‰ä¿¡ç”¨ï¼š%s", self.user["sunshine"])
        logger.info("æ³¨å†Œæ—¶é—´ï¼š%s", self.user["registration_time"])
        logger.info("å¾®åšæ•°ï¼š%d", self.user["statuses_count"])
        logger.info("ç²‰ä¸æ•°ï¼š%d", self.user["followers_count"])
        logger.info("å…³æ³¨æ•°ï¼š%d", self.user["follow_count"])
        logger.info("urlï¼šhttps://m.weibo.cn/profile/%s", self.user["id"])
        if self.user.get("verified_reason"):
            logger.info(self.user["verified_reason"])
        logger.info(self.user["description"])
        logger.info("+" * 100)

    def print_one_weibo(self, weibo):
        """æ‰“å°ä¸€æ¡å¾®åš"""
        try:
            logger.info("å¾®åšidï¼š%d", weibo["id"])
            logger.info("å¾®åšæ­£æ–‡ï¼š%s", weibo["text"])
            logger.info("åŸå§‹å›¾ç‰‡urlï¼š%s", weibo["pics"])
            logger.info("å¾®åšä½ç½®ï¼š%s", weibo["location"])
            logger.info("å‘å¸ƒæ—¶é—´ï¼š%s", weibo["created_at"])
            logger.info("å‘å¸ƒå·¥å…·ï¼š%s", weibo["source"])
            logger.info("ç‚¹èµæ•°ï¼š%d", weibo["attitudes_count"])
            logger.info("è¯„è®ºæ•°ï¼š%d", weibo["comments_count"])
            logger.info("è½¬å‘æ•°ï¼š%d", weibo["reposts_count"])
            logger.info("è¯é¢˜ï¼š%s", weibo["topics"])
            logger.info("@ç”¨æˆ·ï¼š%s", weibo["at_users"])
            logger.info("å·²ç¼–è¾‘ï¼Œç¼–è¾‘æ¬¡æ•°ï¼š%d" % weibo.get("edit_count", 0) if weibo.get("edited") else "æœªç¼–è¾‘")            
            logger.info("urlï¼šhttps://m.weibo.cn/detail/%d", weibo["id"])
        except OSError:
            pass

    def print_weibo(self, weibo):
        """æ‰“å°å¾®åšï¼Œè‹¥ä¸ºè½¬å‘å¾®åšï¼Œä¼šåŒæ—¶æ‰“å°åŸåˆ›å’Œè½¬å‘éƒ¨åˆ†"""
        if weibo.get("retweet"):
            logger.info("*" * 100)
            logger.info("è½¬å‘éƒ¨åˆ†ï¼š")
            self.print_one_weibo(weibo["retweet"])
            logger.info("*" * 100)
            logger.info("åŸåˆ›éƒ¨åˆ†ï¼š")
        self.print_one_weibo(weibo)
        logger.info("-" * 120)

    def get_one_weibo(self, info):
        """è·å–ä¸€æ¡å¾®åšçš„å…¨éƒ¨ä¿¡æ¯"""
        try:
            weibo_info = info["mblog"]
            weibo_id = weibo_info["id"]
            retweeted_status = weibo_info.get("retweeted_status")
            is_long = (
                True if weibo_info.get("pic_num") > 9 else weibo_info.get("isLongText")
            )
            if retweeted_status and retweeted_status.get("id"):  # è½¬å‘
                retweet_id = retweeted_status.get("id")
                is_long_retweet = retweeted_status.get("isLongText")
                if is_long:
                    weibo = self.get_long_weibo(weibo_id)
                    if not weibo:
                        weibo = self.parse_weibo(weibo_info)
                else:
                    weibo = self.parse_weibo(weibo_info)
                if is_long_retweet:
                    retweet = self.get_long_weibo(retweet_id)
                    if not retweet:
                        retweet = self.parse_weibo(retweeted_status)
                else:
                    retweet = self.parse_weibo(retweeted_status)
                (
                    retweet["created_at"],
                    retweet["full_created_at"],
                ) = self.standardize_date(retweeted_status["created_at"])
                weibo["retweet"] = retweet
            else:  # åŸåˆ›
                if is_long:
                    weibo = self.get_long_weibo(weibo_id)
                    if not weibo:
                        weibo = self.parse_weibo(weibo_info)
                else:
                    weibo = self.parse_weibo(weibo_info)
            weibo["created_at"], weibo["full_created_at"] = self.standardize_date(
                weibo_info["created_at"]
            )
            edit_count = weibo_info.get("edit_count", 0)
            weibo["edited"] = edit_count > 0
            weibo["edit_count"] = edit_count
            return weibo
        except Exception as e:
            logger.exception(e)

    def get_weibo_comments(self, weibo, max_count, on_downloaded):
        """
        :weibo standardlized weibo
        :max_count æœ€å¤§å…è®¸ä¸‹è½½æ•°
        :on_downloaded ä¸‹è½½å®Œæˆæ—¶çš„å®ä¾‹æ–¹æ³•å›è°ƒ
        """
        if weibo["comments_count"] == 0:
            return

        logger.info(
            "æ­£åœ¨ä¸‹è½½è¯„è®º å¾®åšid:{id}".format(id=weibo["id"])
        )
        self._get_weibo_comments_cookie(weibo, 0, max_count, None, on_downloaded)

    def get_weibo_reposts(self, weibo, max_count, on_downloaded):
        """
        :weibo standardlized weibo
        :max_count æœ€å¤§å…è®¸ä¸‹è½½æ•°
        :on_downloaded ä¸‹è½½å®Œæˆæ—¶çš„å®ä¾‹æ–¹æ³•å›è°ƒ
        """
        if weibo["reposts_count"] == 0:
            return

        logger.info(
            "æ­£åœ¨ä¸‹è½½è½¬å‘ å¾®åšid:{id}".format(id=weibo["id"])
        )
        self._get_weibo_reposts_cookie(weibo, 0, max_count, 1, on_downloaded)

    def _get_weibo_comments_cookie(
        self, weibo, cur_count, max_count, max_id, on_downloaded
    ):
        """
        :weibo standardlized weibo
        :cur_count  å·²ç»ä¸‹è½½çš„è¯„è®ºæ•°
        :max_count æœ€å¤§å…è®¸ä¸‹è½½æ•°
        :max_id å¾®åšè¿”å›çš„max_idå‚æ•°
        :on_downloaded ä¸‹è½½å®Œæˆæ—¶çš„å®ä¾‹æ–¹æ³•å›è°ƒ
        """
        if cur_count >= max_count:
            return

        id = weibo["id"]
        params = {"mid": id}
        if max_id:
            params["max_id"] = max_id
        url = "https://m.weibo.cn/comments/hotflow?max_id_type=0"
        req = self.session.get(
            url,
            params=params,
            headers=self.headers,
        )
        json = None
        error = False
        try:
            json = req.json()
        except Exception as e:
            # æ²¡æœ‰cookieä¼šæŠ“å–å¤±è´¥
            # å¾®åšæ—¥æœŸå°äºæŸä¸ªæ—¥æœŸçš„ç”¨è¿™ä¸ªurlä¼šè¢«403 éœ€è¦ç”¨è€åŠæ³•å°è¯•ä¸€ä¸‹
            error = True

        if error:
            # æœ€å¤§å¥½åƒåªèƒ½æœ‰50æ¡ TODO: improvement
            self._get_weibo_comments_nocookie(weibo, 0, max_count, 1, on_downloaded)
            return

        data = json.get("data")
        if not data:
            # æ–°æ¥å£æ²¡æœ‰æŠ“å–åˆ°çš„è€æ¥å£ä¹Ÿè¯•ä¸€ä¸‹
            self._get_weibo_comments_nocookie(weibo, 0, max_count, 1, on_downloaded)
            return

        comments = data.get("data")
        count = len(comments)
        if count == 0:
            # æ²¡æœ‰äº†å¯ä»¥ç›´æ¥è·³å‡ºé€’å½’
            return

        if on_downloaded:
            on_downloaded(weibo, comments)

        # éšæœºç¡çœ ä¸€ä¸‹
        if max_count % 40 == 0:
            sleep(random.randint(1, 5))

        cur_count += count
        max_id = data.get("max_id")

        if max_id == 0:
            return

        self._get_weibo_comments_cookie(
            weibo, cur_count, max_count, max_id, on_downloaded
        )

    def _get_weibo_comments_nocookie(
        self, weibo, cur_count, max_count, page, on_downloaded
    ):
        """
        :weibo standardlized weibo
        :cur_count  å·²ç»ä¸‹è½½çš„è¯„è®ºæ•°
        :max_count æœ€å¤§å…è®¸ä¸‹è½½æ•°
        :page ä¸‹è½½çš„é¡µç  ä» 1 å¼€å§‹
        :on_downloaded ä¸‹è½½å®Œæˆæ—¶çš„å®ä¾‹æ–¹æ³•å›è°ƒ
        """
        if cur_count >= max_count:
            return
        id = weibo["id"]
        url = "https://m.weibo.cn/api/comments/show?id={id}&page={page}".format(
            id=id, page=page
        )
        req = self.session.get(url)
        json = None
        try:
            json = req.json()
        except Exception as e:
            logger.warning("æœªèƒ½æŠ“å–å®Œæ•´è¯„è®º å¾®åšid: {id}".format(id=id))
            return

        data = json.get("data")
        if not data:
            return
        comments = data.get("data")
        count = len(comments)
        if count == 0:
            # æ²¡æœ‰äº†å¯ä»¥ç›´æ¥è·³å‡ºé€’å½’
            return

        if on_downloaded:
            on_downloaded(weibo, comments)

        cur_count += count
        page += 1

        # éšæœºç¡çœ ä¸€ä¸‹
        if page % 2 == 0:
            sleep(random.randint(1, 5))

        req_page = data.get("max")

        if req_page == 0:
            return

        if page > req_page:
            return
        self._get_weibo_comments_nocookie(
            weibo, cur_count, max_count, page, on_downloaded
        )

    def _get_weibo_reposts_cookie(
        self, weibo, cur_count, max_count, page, on_downloaded
    ):
        """
        :weibo standardlized weibo
        :cur_count  å·²ç»ä¸‹è½½çš„è½¬å‘æ•°
        :max_count æœ€å¤§å…è®¸ä¸‹è½½æ•°
        :page ä¸‹è½½çš„é¡µç  ä» 1 å¼€å§‹
        :on_downloaded ä¸‹è½½å®Œæˆæ—¶çš„å®ä¾‹æ–¹æ³•å›è°ƒ
        """
        if cur_count >= max_count:
            return
        id = weibo["id"]
        url = "https://m.weibo.cn/api/statuses/repostTimeline"
        params = {"id": id, "page": page}
        req = self.session.get(
            url,
            params=params,
            headers=self.headers,
        )

        json = None
        try:
            json = req.json()
        except Exception as e:
            logger.warning(
                "æœªèƒ½æŠ“å–å®Œæ•´è½¬å‘ å¾®åšid: {id}".format(id=id)
            )
            return

        data = json.get("data")
        if not data:
            return
        reposts = data.get("data")
        count = len(reposts)
        if count == 0:
            # æ²¡æœ‰äº†å¯ä»¥ç›´æ¥è·³å‡ºé€’å½’
            return

        if on_downloaded:
            on_downloaded(weibo, reposts)

        cur_count += count
        page += 1

        # éšæœºç¡çœ ä¸€ä¸‹
        if page % 2 == 0:
            sleep(random.randint(2, 5))

        req_page = data.get("max")

        if req_page == 0:
            return

        if page > req_page:
            return
        self._get_weibo_reposts_cookie(weibo, cur_count, max_count, page, on_downloaded)



    def get_one_page(self, page):
        """è·å–ä¸€é¡µçš„å…¨éƒ¨å¾®åš"""
        try:
            js = self.get_weibo_json(page)
            if js["ok"]:
                weibos = js["data"]["cards"]
                
                if self.query:
                    weibos = weibos[0]["card_group"]
                # å¦‚æœéœ€è¦æ£€æŸ¥cookieï¼Œåœ¨å¾ªç¯ç¬¬ä¸€ä¸ªäººçš„æ—¶å€™ï¼Œå°±è¦çœ‹çœ‹ä»…è‡ªå·±å¯è§çš„ä¿¡æ¯æœ‰æ²¡æœ‰ï¼Œè¦æ˜¯æ²¡æœ‰ç›´æ¥æŠ¥é”™
                for w in weibos:
                    if w["card_type"] == 11:
                        temp = w.get("card_group",[0])
                        if len(temp) >= 1:
                            w = temp[0] or w
                        else:
                            w = w
                    if w["card_type"] == 9:
                        wb = self.get_one_weibo(w)
                        if wb:
                            if (
                                const.CHECK_COOKIE["CHECK"]
                                and (not const.CHECK_COOKIE["CHECKED"])
                                and wb["text"].startswith(
                                    const.CHECK_COOKIE["HIDDEN_WEIBO"]
                                )
                            ):
                                const.CHECK_COOKIE["CHECKED"] = True
                                logger.info("cookieæ£€æŸ¥é€šè¿‡")
                                if const.CHECK_COOKIE["EXIT_AFTER_CHECK"]:
                                    return True
                            if wb["id"] in self.weibo_id_list:
                                continue
                            created_at = datetime.strptime(wb["created_at"], DTFORMAT)
                            since_date = datetime.strptime(
                                self.user_config["since_date"], DTFORMAT
                            )
                            if const.MODE == "append":
                                # appendæ¨¡å¼ï¼šå¢é‡è·å–å¾®åš
                                if self.first_crawler:
                                    # è®°å½•æœ€æ–°å¾®åšidï¼Œå†™å…¥ä¸Šæ¬¡æŠ“å–idçš„csv
                                    self.latest_weibo_id = str(wb["id"])
                                    csvutil.update_last_weibo_id(
                                        wb["user_id"],
                                        str(wb["id"]) + " " + wb["created_at"],
                                        self.user_csv_file_path,
                                    )
                                    self.first_crawler = False
                                if str(wb["id"]) == self.last_weibo_id:
                                    if const.CHECK_COOKIE["CHECK"] and (
                                        not const.CHECK_COOKIE["CHECKED"]
                                    ):
                                        # å·²ç»çˆ¬å–è¿‡æœ€æ–°çš„äº†ï¼Œåªæ˜¯æ²¡æ£€æŸ¥åˆ°cookieï¼Œä¸€æ—¦æ£€æŸ¥é€šè¿‡ï¼Œç›´æ¥æ”¾è¡Œ
                                        const.CHECK_COOKIE["EXIT_AFTER_CHECK"] = True
                                        continue
                                    if self.last_weibo_id == self.latest_weibo_id:
                                        logger.info(
                                            "{} ç”¨æˆ·æ²¡æœ‰å‘æ–°å¾®åš".format(
                                                self.user["screen_name"]
                                            )
                                        )
                                    else:
                                        logger.info(
                                            "å¢é‡è·å–å¾®åšå®Œæ¯•ï¼Œå°†æœ€æ–°å¾®åšidä» {} å˜æ›´ä¸º {}".format(
                                                self.last_weibo_id, self.latest_weibo_id
                                            )
                                        )
                                    return True
                                # ä¸Šä¸€æ¬¡æ ‡è®°çš„å¾®åšè¢«åˆ äº†ï¼Œå°±æŠŠä¸Šä¸€æ¡å¾®åšæ—¶é—´è®°å½•æ¨å‰ä¸¤å¤©ï¼Œå¤šæŠ“ç‚¹è¯„è®ºæˆ–è€…å¾®åšå†…å®¹ä¿®æ”¹
                                # TODO æ›´åŠ åˆç†çš„æµç¨‹æ˜¯ï¼Œå³ä½¿è¯»å–åˆ°ä¸Šæ¬¡æ›´æ–°å¾®åšidï¼Œä¹ŸæŠ“å–å¢é‡è¯„è®ºï¼Œç”±æ­¤è·å¾—æ›´å¤šçš„è¯„è®º
                                since_date = datetime.strptime(
                                    convert_to_days_ago(self.last_weibo_date, 1),
                                    DTFORMAT,
                                )
                            if created_at < since_date:
                                # æ£€æŸ¥æ˜¯å¦ä¸ºç½®é¡¶å¾®åš
                                is_pinned = w.get("mblog", {}).get("mblogtype", 0) == 2
                                if is_pinned:
                                    logger.debug(f"[ç½®é¡¶å¾®åš] å¾®åšID={wb['id']}, å‘å¸ƒæ—¶é—´={created_at}, æ˜¯ç½®é¡¶å¾®åšï¼Œè·³è¿‡ä½†ç»§ç»­æ£€æŸ¥åç»­å¾®åš")
                                    continue
                                
                                logger.debug(f"[æ—¥æœŸè¿‡æ»¤] å¾®åšID={wb['id']}, å‘å¸ƒæ—¶é—´={created_at}, èµ·å§‹æ—¶é—´={since_date}, è¢«è·³è¿‡")
                                # å¦‚æœè¦æ£€æŸ¥è¿˜æ²¡æœ‰æ£€æŸ¥cookieï¼Œä¸èƒ½ç›´æ¥è·³å‡º
                                if const.CHECK_COOKIE["CHECK"] and (
                                    not const.CHECK_COOKIE["CHECKED"]
                                ):
                                    continue
                                else:
                                    logger.info(
                                        "{}å·²è·å–{}({})çš„ç¬¬{}é¡µ{}å¾®åš{}".format(
                                            "-" * 30,
                                            self.user["screen_name"],
                                            self.user["id"],
                                            page,
                                            'åŒ…å«"' + self.query + '"çš„'
                                            if self.query
                                            else "",
                                            "-" * 30,
                                        )
                                    )
                                    return True
                            else:
                                logger.debug(f"[æ—¥æœŸé€šè¿‡] å¾®åšID={wb['id']}, å‘å¸ƒæ—¶é—´={created_at}, èµ·å§‹æ—¶é—´={since_date}")
                            if (not self.only_crawl_original) or ("retweet" not in wb.keys()):
                                self.weibo.append(wb)
                                self.weibo_id_list.append(wb["id"])
                                self.got_count += 1

                                # é˜²å°ç¦ï¼šæ›´æ–°å¾®åšç»Ÿè®¡
                                self.update_crawl_stats(weibo_count=1)

                                # é˜²å°ç¦ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦æš‚åœ
                                if self.anti_ban_enabled:
                                    should_pause, reason = self.should_pause_session()
                                    if should_pause:
                                        logger.warning(f"è§¦å‘é˜²å°ç¦æš‚åœ: {reason}")
                                        return "need_rest"  # è¿”å›ç‰¹æ®Šå€¼è¡¨ç¤ºéœ€è¦ä¼‘æ¯

                                # è¿™é‡Œæ˜¯ç³»ç»Ÿæ—¥å¿—è¾“å‡ºï¼Œå°½é‡åˆ«å¤ªæ‚
                                logger.info(
                                    "å·²è·å–ç”¨æˆ· {} çš„å¾®åšï¼Œå†…å®¹ä¸º {}".format(
                                        self.user["screen_name"], wb["text"]
                                    )
                                )
                                # self.print_weibo(wb)
                            else:
                                logger.info("æ­£åœ¨è¿‡æ»¤è½¬å‘å¾®åš")
                    
                if const.CHECK_COOKIE["CHECK"] and not const.CHECK_COOKIE["CHECKED"]:
                    logger.warning("ç»æ£€æŸ¥ï¼Œcookieæ— æ•ˆï¼Œç³»ç»Ÿé€€å‡º")
                    if const.NOTIFY["NOTIFY"]:
                        push_deer("ç»æ£€æŸ¥ï¼Œcookieæ— æ•ˆï¼Œç³»ç»Ÿé€€å‡º")
                    sys.exit()
            else:
                return True
            logger.info(
                "{}å·²è·å–{}({})çš„ç¬¬{}é¡µå¾®åš{}".format(
                    "-" * 30, self.user["screen_name"], self.user["id"], page, "-" * 30
                )
            )
        except Exception as e:
            logger.exception(e)

    def get_page_count(self):
        """è·å–å¾®åšé¡µæ•°"""
        try:
            weibo_count = self.user["statuses_count"]
            page_weibo_count = self.page_weibo_count
            page_count = int(math.ceil(weibo_count / page_weibo_count))
            if not isinstance(page_weibo_count, int):
                raise ValueError("config.jsonä¸­æ¯é¡µçˆ¬å–çš„å¾®åšæ•° page_weibo_count å¿…é¡»æ˜¯ä¸€ä¸ªæ•´æ•°")
            return page_count
        except KeyError:
            logger.exception(
                "ç¨‹åºå‡ºé”™ï¼Œé”™è¯¯åŸå› å¯èƒ½ä¸ºä»¥ä¸‹ä¸¤è€…ï¼š\n"
                "1.user_idä¸æ­£ç¡®ï¼›\n"
                "2.æ­¤ç”¨æˆ·å¾®åšå¯èƒ½éœ€è¦è®¾ç½®cookieæ‰èƒ½çˆ¬å–ã€‚\n"
                "è§£å†³æ–¹æ¡ˆï¼š\n"
                "è¯·å‚è€ƒ\n"
                "https://github.com/dataabc/weibo-crawler#å¦‚ä½•è·å–user_id\n"
                "è·å–æ­£ç¡®çš„user_idï¼›\n"
                "æˆ–è€…å‚è€ƒ\n"
                "https://github.com/dataabc/weibo-crawler#3ç¨‹åºè®¾ç½®\n"
                "ä¸­çš„â€œè®¾ç½®cookieâ€éƒ¨åˆ†è®¾ç½®cookieä¿¡æ¯"
            )

    def get_write_info(self, wrote_count):
        """è·å–è¦å†™å…¥çš„å¾®åšä¿¡æ¯"""
        write_info = []
        for w in self.weibo[wrote_count:]:
            wb = OrderedDict()
            for k, v in w.items():
                if k not in ["user_id", "screen_name", "retweet"]:
                    if "unicode" in str(type(v)):
                        v = v.encode("utf-8")
                    if k == "id":
                        v = str(v) + "\t"
                    wb[k] = v
            if not self.only_crawl_original:
                if w.get("retweet"):
                    wb["is_original"] = False
                    for k2, v2 in w["retweet"].items():
                        if "unicode" in str(type(v2)):
                            v2 = v2.encode("utf-8")
                        if k2 == "id":
                            v2 = str(v2) + "\t"
                        wb["retweet_" + k2] = v2
                else:
                    wb["is_original"] = True
            write_info.append(wb)
        return write_info

    def get_filepath(self, type):
        """è·å–ç»“æœæ–‡ä»¶è·¯å¾„"""
        try:
            dir_name = self.user["screen_name"]
            if self.user_id_as_folder_name:
                dir_name = str(self.user_config["user_id"])
            file_dir = (
                os.path.split(os.path.realpath(__file__))[0]
                + os.sep
                + "weibo"
                + os.sep
                + dir_name
            )
            if type in ["img", "video", "live_photo"]:
                file_dir = file_dir + os.sep + type
            elif type == "markdown":
                # Markdownæ–‡ä»¶ä¿å­˜åœ¨ç”¨æˆ·ç›®å½•ä¸‹ï¼Œå›¾ç‰‡åœ¨ç”¨æˆ·ç›®å½•çš„imgå­ç›®å½•ä¸­
                file_dir = file_dir
            if not os.path.isdir(file_dir):
                os.makedirs(file_dir)
            if type in ["img", "video", "live_photo"]:
                return file_dir
            elif type == "markdown":
                # å¯¹äºmarkdownç±»å‹ï¼Œè¿”å›ç›®å½•è·¯å¾„ï¼Œæ–‡ä»¶åä¼šåœ¨generate_markdown_fileä¸­æŒ‡å®š
                return file_dir
            file_path = file_dir + os.sep + str(self.user_config["user_id"]) + "." + type
            return file_path
        except Exception as e:
            logger.exception(e)

    def get_result_headers(self):
        """è·å–è¦å†™å…¥ç»“æœæ–‡ä»¶çš„è¡¨å¤´"""
        result_headers = [
            "id",
            "bid",
            "æ­£æ–‡",
            "å¤´æ¡æ–‡ç« url",
            "åŸå§‹å›¾ç‰‡url",
            "è§†é¢‘url",
            "Live Photoè§†é¢‘url",
            "ä½ç½®",
            "æ—¥æœŸ",
            "å·¥å…·",
            "ç‚¹èµæ•°",
            "è¯„è®ºæ•°",
            "è½¬å‘æ•°",
            "è¯é¢˜",
            "@ç”¨æˆ·",
            "å®Œæ•´æ—¥æœŸ",
            "æ˜¯å¦ç¼–è¾‘è¿‡",
            "ç¼–è¾‘æ¬¡æ•°",            
        ]
        if not self.only_crawl_original:
            result_headers2 = ["æ˜¯å¦åŸåˆ›", "æºç”¨æˆ·id", "æºç”¨æˆ·æ˜µç§°"]
            result_headers3 = ["æºå¾®åš" + r for r in result_headers]
            result_headers = result_headers + result_headers2 + result_headers3
        return result_headers

    def write_csv(self, wrote_count):
        """å°†çˆ¬åˆ°çš„ä¿¡æ¯å†™å…¥csvæ–‡ä»¶"""
        write_info = self.get_write_info(wrote_count)
        result_headers = self.get_result_headers()
        result_data = [w.values() for w in write_info]
        file_path = self.get_filepath("csv")
        self.csv_helper(result_headers, result_data, file_path)

    def csv_helper(self, headers, result_data, file_path):
        """å°†æŒ‡å®šä¿¡æ¯å†™å…¥csvæ–‡ä»¶"""
        if not os.path.isfile(file_path):
            is_first_write = 1
        else:
            is_first_write = 0
        if sys.version < "3":  # python2.x
            with open(file_path, "ab") as f:
                f.write(codecs.BOM_UTF8)
                writer = csv.writer(f)
                if is_first_write:
                    writer.writerows([headers])
                writer.writerows(result_data)
        else:  # python3.x

            with open(file_path, "a", encoding="utf-8-sig", newline="") as f:
                writer = csv.writer(f)
                if is_first_write:
                    writer.writerows([headers])
                writer.writerows(result_data)
        if headers[0] == "id":
            logger.info("%dæ¡å¾®åšå†™å…¥csvæ–‡ä»¶å®Œæ¯•,ä¿å­˜è·¯å¾„:", self.got_count)
        else:
            logger.info("%s ä¿¡æ¯å†™å…¥csvæ–‡ä»¶å®Œæ¯•ï¼Œä¿å­˜è·¯å¾„:", self.user["screen_name"])
        logger.info(file_path)

    def update_json_data(self, data, weibo_info):
        """æ›´æ–°è¦å†™å…¥jsonç»“æœæ–‡ä»¶ä¸­çš„æ•°æ®ï¼Œå·²ç»å­˜åœ¨äºjsonä¸­çš„ä¿¡æ¯æ›´æ–°ä¸ºæœ€æ–°å€¼ï¼Œä¸å­˜åœ¨çš„ä¿¡æ¯æ·»åŠ åˆ°dataä¸­"""
        data["user"] = self.user
        if data.get("weibo"):
            is_new = 1  # å¾…å†™å…¥å¾®åšæ˜¯å¦å…¨éƒ¨ä¸ºæ–°å¾®åšï¼Œå³å¾…å†™å…¥å¾®åšä¸jsonä¸­çš„æ•°æ®ä¸é‡å¤
            for old in data["weibo"]:
                if weibo_info[-1]["id"] == old["id"]:
                    is_new = 0
                    break
            if is_new == 0:
                for new in weibo_info:
                    flag = 1
                    for i, old in enumerate(data["weibo"]):
                        if new["id"] == old["id"]:
                            data["weibo"][i] = new
                            flag = 0
                            break
                    if flag:
                        data["weibo"].append(new)
            else:
                data["weibo"] += weibo_info
        else:
            data["weibo"] = weibo_info
        return data

    def write_json(self, wrote_count):
        """å°†çˆ¬åˆ°çš„ä¿¡æ¯å†™å…¥jsonæ–‡ä»¶"""
        data = {}
        path = self.get_filepath("json")
        if os.path.isfile(path):
            with codecs.open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
        weibo_info = self.weibo[wrote_count:]
        data = self.update_json_data(data, weibo_info)
        with codecs.open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False)
        logger.info("%dæ¡å¾®åšå†™å…¥jsonæ–‡ä»¶å®Œæ¯•,ä¿å­˜è·¯å¾„:", self.got_count)
        logger.info(path)

    def send_post_request_with_token(self, url, data, token, max_retries, backoff_factor):
        headers = {
            'Content-Type': 'application/json',
            'api-token': f'{token}',
        }
        for attempt in range(max_retries + 1):
            try:
                response = self.session.get(url, json=data, headers=headers)
                if response.status_code == requests.codes.ok:
                    return response.json()
                else:
                    raise RequestException(f"Unexpected response status: {response.status_code}")
            except RequestException as e:
                if attempt < max_retries:
                    sleep(backoff_factor * (attempt + 1))  # é€æ­¥å¢åŠ ç­‰å¾…æ—¶é—´ï¼Œé¿å…é¢‘ç¹é‡è¯•
                    continue
                else:
                    logger.error(f"åœ¨å°è¯•{max_retries}æ¬¡å‘å‡ºPOSTè¿æ¥åï¼Œè¯·æ±‚å¤±è´¥ï¼š{e}")

    def write_post(self, wrote_count):
        """å°†çˆ¬åˆ°çš„ä¿¡æ¯é€šè¿‡POSTå‘å‡º"""
        data = {}
        data['user'] = self.user
        weibo_info = self.weibo[wrote_count:]
        if data.get('weibo'):
            data['weibo'] += weibo_info
        else:
            data['weibo'] = weibo_info

        if data:
            self.send_post_request_with_token(self.post_config["api_url"], data, self.post_config["api_token"], 3, 2)
            logger.info(u'%dæ¡å¾®åšé€šè¿‡POSTå‘é€åˆ° %s', len(data['weibo']), self.post_config["api_url"])
        else:
            logger.info(u'æ²¡æœ‰è·å–åˆ°å¾®åšï¼Œç•¥è¿‡API POST')


    def info_to_mongodb(self, collection, info_list):
        """å°†çˆ¬å–çš„ä¿¡æ¯å†™å…¥MongoDBæ•°æ®åº“"""
        try:
            import pymongo
        except ImportError:
            logger.warning("ç³»ç»Ÿä¸­å¯èƒ½æ²¡æœ‰å®‰è£…pymongoåº“ï¼Œè¯·å…ˆè¿è¡Œ pip install pymongo ï¼Œå†è¿è¡Œç¨‹åº")
            sys.exit()
        try:
            from pymongo import MongoClient

            client = MongoClient(self.mongodb_URI)
            db = client["weibo"]
            collection = db[collection]
            if len(self.write_mode) > 1:
                new_info_list = copy.deepcopy(info_list)
            else:
                new_info_list = info_list
            for info in new_info_list:
                if not collection.find_one({"id": info["id"]}):
                    collection.insert_one(info)
                else:
                    collection.update_one({"id": info["id"]}, {"$set": info})
        except pymongo.errors.ServerSelectionTimeoutError:
            logger.warning("ç³»ç»Ÿä¸­å¯èƒ½æ²¡æœ‰å®‰è£…æˆ–å¯åŠ¨MongoDBæ•°æ®åº“ï¼Œè¯·å…ˆæ ¹æ®ç³»ç»Ÿç¯å¢ƒå®‰è£…æˆ–å¯åŠ¨MongoDBï¼Œå†è¿è¡Œç¨‹åº")
            sys.exit()

    def weibo_to_mongodb(self, wrote_count):
        """å°†çˆ¬å–çš„å¾®åšä¿¡æ¯å†™å…¥MongoDBæ•°æ®åº“"""
        self.info_to_mongodb("weibo", self.weibo[wrote_count:])
        logger.info("%dæ¡å¾®åšå†™å…¥MongoDBæ•°æ®åº“å®Œæ¯•", self.got_count)

    def mysql_create(self, connection, sql):
        """åˆ›å»ºMySQLæ•°æ®åº“æˆ–è¡¨"""
        try:
            with connection.cursor() as cursor:
                cursor.execute(sql)
        finally:
            connection.close()

    def mysql_create_database(self, mysql_config, sql):
        """åˆ›å»ºMySQLæ•°æ®åº“"""
        try:
            import pymysql
        except ImportError:
            logger.warning("ç³»ç»Ÿä¸­å¯èƒ½æ²¡æœ‰å®‰è£…pymysqlåº“ï¼Œè¯·å…ˆè¿è¡Œ pip install pymysql ï¼Œå†è¿è¡Œç¨‹åº")
            sys.exit()
        try:
            if self.mysql_config:
                mysql_config = self.mysql_config
            connection = pymysql.connect(**mysql_config)
            self.mysql_create(connection, sql)
        except pymysql.OperationalError:
            logger.warning("ç³»ç»Ÿä¸­å¯èƒ½æ²¡æœ‰å®‰è£…æˆ–æ­£ç¡®é…ç½®MySQLæ•°æ®åº“ï¼Œè¯·å…ˆæ ¹æ®ç³»ç»Ÿç¯å¢ƒå®‰è£…æˆ–é…ç½®MySQLï¼Œå†è¿è¡Œç¨‹åº")
            sys.exit()

    def mysql_create_table(self, mysql_config, sql):
        """åˆ›å»ºMySQLè¡¨"""
        import pymysql

        if self.mysql_config:
            mysql_config = self.mysql_config
        mysql_config["db"] = "weibo"
        connection = pymysql.connect(**mysql_config)
        self.mysql_create(connection, sql)

    def mysql_insert(self, mysql_config, table, data_list):
        """
        å‘MySQLè¡¨æ’å…¥æˆ–æ›´æ–°æ•°æ®

        Parameters
        ----------
        mysql_config: map
            MySQLé…ç½®è¡¨
        table: str
            è¦æ’å…¥çš„è¡¨å
        data_list: list
            è¦æ’å…¥çš„æ•°æ®åˆ—è¡¨

        Returns
        -------
        bool: SQLæ‰§è¡Œç»“æœ
        """
        import pymysql

        if len(data_list) > 0:
            keys = ", ".join(data_list[0].keys())
            values = ", ".join(["%s"] * len(data_list[0]))
            if self.mysql_config:
                mysql_config = self.mysql_config
            mysql_config["db"] = "weibo"
            connection = pymysql.connect(**mysql_config)
            cursor = connection.cursor()
            sql = """INSERT INTO {table}({keys}) VALUES ({values}) ON
                     DUPLICATE KEY UPDATE""".format(
                table=table, keys=keys, values=values
            )
            update = ",".join(
                [" {key} = values({key})".format(key=key) for key in data_list[0]]
            )
            sql += update
            try:
                cursor.executemany(sql, [tuple(data.values()) for data in data_list])
                connection.commit()
            except Exception as e:
                connection.rollback()
                logger.exception(e)
            finally:
                connection.close()

    def weibo_to_mysql(self, wrote_count):
        """å°†çˆ¬å–çš„å¾®åšä¿¡æ¯å†™å…¥MySQLæ•°æ®åº“"""
        mysql_config = {
            "host": "localhost",
            "port": 3306,
            "user": "root",
            "password": "123456",
            "charset": "utf8mb4",
        }
        # åˆ›å»º'weibo'è¡¨
        create_table = """
                CREATE TABLE IF NOT EXISTS weibo (
                id varchar(20) NOT NULL,
                bid varchar(12) NOT NULL,
                user_id varchar(20),
                screen_name varchar(30),
                text text,
                article_url varchar(100),
                topics varchar(200),
                at_users varchar(1000),
                pics varchar(3000),
                video_url varchar(1000),
                live_photo_url varchar(1000),
                location varchar(100),
                created_at DATETIME,
                source varchar(30),
                attitudes_count INT,
                comments_count INT,
                reposts_count INT,
                retweet_id varchar(20),
                edited BOOLEAN DEFAULT 0,
                edit_count INT DEFAULT 0,
                PRIMARY KEY (id)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4"""
        self.mysql_create_table(mysql_config, create_table)

        # è¦æ’å…¥çš„å¾®åšåˆ—è¡¨
        weibo_list = []
        # è¦æ’å…¥çš„è½¬å‘å¾®åšåˆ—è¡¨
        retweet_list = []
        if len(self.write_mode) > 1:
            info_list = copy.deepcopy(self.weibo[wrote_count:])
        else:
            info_list = self.weibo[wrote_count:]
        for w in info_list:
            w["created_at"] = w["full_created_at"]
            del w["full_created_at"]

            if "retweet" in w:
                r = w["retweet"]
                r["retweet_id"] = ""
                r["created_at"] = r["full_created_at"]
                del r["full_created_at"]
                retweet_list.append(r)
                w["retweet_id"] = r["id"]
                del w["retweet"]
            else:
                w["retweet_id"] = ""
            weibo_list.append(w)
        # åœ¨'weibo'è¡¨ä¸­æ’å…¥æˆ–æ›´æ–°å¾®åšæ•°æ®
        self.mysql_insert(mysql_config, "weibo", retweet_list)
        self.mysql_insert(mysql_config, "weibo", weibo_list)
        logger.info("%dæ¡å¾®åšå†™å…¥MySQLæ•°æ®åº“å®Œæ¯•", self.got_count)

    def weibo_to_sqlite(self, wrote_count):
        con = self.get_sqlite_connection()
        weibo_list = []
        retweet_list = []
        info_list = copy.deepcopy(self.weibo[wrote_count:])
        for w in info_list:
            if "retweet" in w:
                w["retweet"]["retweet_id"] = ""
                retweet_list.append(w["retweet"])
                w["retweet_id"] = w["retweet"]["id"]
                del w["retweet"]
            else:
                w["retweet_id"] = ""
            weibo_list.append(w)

        comment_max_count = self.comment_max_download_count
        repost_max_count = self.repost_max_download_count
        download_comment = self.download_comment and comment_max_count > 0
        download_repost = self.download_repost and repost_max_count > 0

        count = 0
        for weibo in weibo_list:
            self.sqlite_insert_weibo(con, weibo)
            if (download_comment) and (weibo["comments_count"] > 0):
                self.get_weibo_comments(
                    weibo, comment_max_count, self.sqlite_insert_comments
                )
                count += 1
                if count % 20:
                    sleep(random.randint(3, 6))
            if (download_repost) and (weibo["reposts_count"] > 0):
                self.get_weibo_reposts(
                    weibo, repost_max_count, self.sqlite_insert_reposts
                )
                count += 1
                if count % 20:
                    sleep(random.randint(3, 6))

        for weibo in retweet_list:
            self.sqlite_insert_weibo(con, weibo)
        con.close()

    def export_comments_to_csv_for_current_user(self):
        """å°†å½“å‰ç”¨æˆ·ç›¸å…³çš„è¯„è®ºä» SQLite å¯¼å‡ºåˆ°è¯¥ç”¨æˆ·ç›®å½•ä¸‹çš„ CSV æ–‡ä»¶"""
        # ä»…åœ¨å¯ç”¨äº† sqlite å†™å…¥ä¸”å¼€å¯ä¸‹è½½è¯„è®ºæ—¶å¯¼å‡º
        if "sqlite" not in self.write_mode or not self.download_comment:
            return
        try:
            db_path = self.get_sqlte_path()
            if not os.path.exists(db_path):
                logger.warning("å¯¼å‡ºè¯„è®ºå¤±è´¥ï¼Œæœªæ‰¾åˆ°SQLiteæ•°æ®åº“: %s", db_path)
                return

            # å½“å‰ç”¨æˆ·çš„ IDï¼Œç”¨äºç­›é€‰å±äºè¯¥ç”¨æˆ·å¾®åšçš„è¯„è®º
            user_id = str(self.user_config.get("user_id", ""))
            if not user_id:
                logger.warning("å¯¼å‡ºè¯„è®ºå¤±è´¥ï¼Œå½“å‰ç”¨æˆ·IDä¸ºç©º")
                return

            # ç”¨æˆ·ç»“æœç›®å½•ï¼Œä¸å¾®åš CSV åŒçº§ï¼Œä¾‹å¦‚ weibo/èƒ¡æ­Œ/ æˆ– weibo/1223178222/
            csv_path = self.get_filepath("csv")
            user_dir = os.path.dirname(csv_path)
            if not os.path.isdir(user_dir):
                os.makedirs(user_dir)
            # ä½¿ç”¨ç”¨æˆ·æ˜µç§°ä½œä¸ºæ–‡ä»¶åçš„ä¸€éƒ¨åˆ†ï¼Œé¿å…å†å‡ºç°çº¯æ•°å­— user_id
            screen_name = self.user.get("screen_name") or user_id
            safe_screen_name = re.sub(r'[\\/:*?"<>|]', "_", str(screen_name))
            out_path = os.path.join(user_dir, f"{safe_screen_name}_comments.csv")

            con = sqlite3.connect(db_path)
            cur = con.cursor()

            # åªå¯¼å‡ºå½“å‰ç”¨æˆ·å¾®åšä¸‹çš„è¯„è®º
            sql = """
                SELECT
                    c.id,
                    c.weibo_id,
                    c.created_at,
                    c.user_screen_name,
                    c.text,
                    c.pic_url,
                    c.like_count
                FROM comments c
                JOIN weibo w ON c.weibo_id = w.id
                WHERE w.user_id = ?
                ORDER BY c.weibo_id, c.id
            """
            rows = cur.execute(sql, (user_id,)).fetchall()
            con.close()

            if not rows:
                logger.info("ç”¨æˆ· %s æ²¡æœ‰å¯å¯¼å‡ºçš„è¯„è®ºè®°å½•ï¼Œè·³è¿‡ç”Ÿæˆè¯„è®º CSV", user_id)
                return

            header = [
                "id",
                "weibo_id",
                "created_at",
                "user_screen_name",
                "text",
                "pic_url",
                "like_count",
            ]

            # 1ï¼‰å¯¼å‡ºå½“å‰ç”¨æˆ·çš„æ±‡æ€»è¯„è®ºæ–‡ä»¶ï¼š<ç”¨æˆ·æ˜µç§°>_comments.csv
            with open(out_path, "w", newline="", encoding="utf-8-sig") as f:
                writer = csv.writer(f)
                writer.writerow(header)
                writer.writerows(rows)

            # 2ï¼‰æŒ‰æ¯æ¡å¾®åšæ‹†åˆ†å¯¼å‡ºï¼š<ç”¨æˆ·æ˜µç§°>_<weibo_id>_comments.csv
            #    æ»¡è¶³â€œç”¨æˆ·æ˜µç§° + weiboId + commentsâ€çš„æ–‡ä»¶å‘½åè¦æ±‚
            comments_by_weibo = {}
            for row in rows:
                weibo_id = row[1]
                comments_by_weibo.setdefault(weibo_id, []).append(row)

            for weibo_id, weibo_rows in comments_by_weibo.items():
                per_weibo_path = os.path.join(
                    user_dir, f"{safe_screen_name}_{weibo_id}_comments.csv"
                )
                with open(per_weibo_path, "w", newline="", encoding="utf-8-sig") as f:
                    writer = csv.writer(f)
                    writer.writerow(header)
                    writer.writerows(weibo_rows)

            logger.info(
                "å…±å¯¼å‡º %d æ¡è¯„è®ºåˆ°ç”¨æˆ·æ±‡æ€» CSV: %sï¼Œå¹¶æŒ‰æ¯æ¡å¾®åšæ‹†åˆ†ç”Ÿæˆ %d ä¸ªè¯„è®º CSV",
                len(rows),
                out_path,
                len(comments_by_weibo),
            )
        except Exception as e:
            logger.exception(e)

    def sqlite_insert_comments(self, weibo, comments):
        if not comments or len(comments) == 0:
            return
        con = self.get_sqlite_connection()
        for comment in comments:
            data = self.parse_sqlite_comment(comment, weibo)
            self.sqlite_insert(con, data, "comments")
            if "comments" in comment and isinstance(comment["comments"], list):
                for c in comment["comments"]:
                    data = self.parse_sqlite_comment(c, weibo)
                    self.sqlite_insert(con, data, "comments")
        con.close()

    def sqlite_insert_reposts(self, weibo, reposts):
        if not reposts or len(reposts) == 0:
            return
        con = self.get_sqlite_connection()
        for repost in reposts:
            data = self.parse_sqlite_repost(repost, weibo)
            self.sqlite_insert(con, data, "reposts")

        con.close()

    def parse_sqlite_comment(self, comment, weibo):
        if not comment:
            return
        sqlite_comment = OrderedDict()
        sqlite_comment["id"] = comment["id"]

        self._try_get_value("bid", "bid", sqlite_comment, comment)
        self._try_get_value("root_id", "rootid", sqlite_comment, comment)
        self._try_get_value("created_at", "created_at", sqlite_comment, comment)
        sqlite_comment["weibo_id"] = weibo["id"]

        sqlite_comment["user_id"] = comment["user"]["id"]
        sqlite_comment["user_screen_name"] = comment["user"]["screen_name"]
        self._try_get_value(
            "user_avatar_url", "avatar_hd", sqlite_comment, comment["user"]
        )
        if self.remove_html_tag:
            sqlite_comment["text"] = re.sub('<[^<]+?>', '', comment["text"]).replace('\n', '').strip()
        else:
            sqlite_comment["text"] = comment["text"]
        
        sqlite_comment["pic_url"] = ""
        if comment.get("pic"):
            sqlite_comment["pic_url"] = comment["pic"]["large"]["url"]
        if sqlite_comment["pic_url"]:
            pic_url = sqlite_comment["pic_url"]

            # è¯„è®ºå›¾ç‰‡ç›®å½•ï¼šweibo/<ç”¨æˆ·ç›®å½•>/<ç”¨æˆ·æ˜µç§°>_comments_img
            csv_path = self.get_filepath("csv")
            user_dir = os.path.dirname(csv_path)
            if not os.path.isdir(user_dir):
                os.makedirs(user_dir)
            screen_name = self.user.get("screen_name") or str(
                self.user_config.get("user_id", "")
            )
            safe_screen_name = re.sub(r'[\\/:*?"<>|]', "_", str(screen_name))
            pic_path = os.path.join(user_dir, f"{safe_screen_name}_comments_img")
            if not os.path.exists(pic_path):
                os.makedirs(pic_path)

            # æ–‡ä»¶ååŒ…å« å¾®åšç”¨æˆ·æ˜µç§° + weibo_id + è¯„è®ºç”¨æˆ·æ˜µç§° + comments
            # ä¸ºé¿å…é‡åï¼Œå¦‚æœå·²å­˜åœ¨åˆ™åœ¨æœ«å°¾è¿½åŠ  _1/_2/... åºå·
            weibo_id = sqlite_comment["weibo_id"]
            comment_user = sqlite_comment.get("user_screen_name", "")
            safe_comment_user = re.sub(r'[\\/:*?"<>|]', "_", str(comment_user))
            base_name = "{screen_name}_{weibo_id}_{comment_user}_comments".format(
                screen_name=safe_screen_name,
                weibo_id=weibo_id,
                comment_user=safe_comment_user,
            )
            pic_name = base_name + ".jpg"
            idx = 1
            while os.path.exists(os.path.join(pic_path, pic_name)):
                pic_name = f"{base_name}_{idx}.jpg"
                idx += 1
            pic_full_path = os.path.join(pic_path, pic_name)
            if not os.path.exists(pic_full_path):
                try:
                    response = self.session.get(pic_url, timeout=10)
                    with open(pic_full_path, "wb") as f:
                        f.write(response.content)
                    logger.info("è¯„è®ºå›¾ç‰‡ä¸‹è½½æˆåŠŸ: %s", pic_full_path)
                except Exception as e:
                    logger.warning("ä¸‹è½½è¯„è®ºå›¾ç‰‡å¤±è´¥: %s", e)
        self._try_get_value("like_count", "like_count", sqlite_comment, comment)
        return sqlite_comment

    def parse_sqlite_repost(self, repost, weibo):
        if not repost:
            return
        sqlite_repost = OrderedDict()
        sqlite_repost["id"] = repost["id"]

        self._try_get_value("bid", "bid", sqlite_repost, repost)
        self._try_get_value("created_at", "created_at", sqlite_repost, repost)
        sqlite_repost["weibo_id"] = weibo["id"]

        sqlite_repost["user_id"] = repost["user"]["id"]
        sqlite_repost["user_screen_name"] = repost["user"]["screen_name"]
        self._try_get_value(
            "user_avatar_url", "profile_image_url", sqlite_repost, repost["user"]
        )
        text = repost.get("raw_text")
        if text:
            text = text.split("//", 1)[0]
        if text is None or text == "" or text == "Repost":
            text = "è½¬å‘å¾®åš"
        sqlite_repost["text"] = text
        self._try_get_value("like_count", "attitudes_count", sqlite_repost, repost)
        return sqlite_repost

    def _try_get_value(self, source_name, target_name, dict, json):
        dict[source_name] = ""
        value = json.get(target_name)
        if value:
            dict[source_name] = value

    def sqlite_insert_weibo(self, con: sqlite3.Connection, weibo: dict):
        sqlite_weibo = self.parse_sqlite_weibo(weibo)
        self.sqlite_insert(con, sqlite_weibo, "weibo")

    def parse_sqlite_weibo(self, weibo):
        if not weibo:
            return
        sqlite_weibo = OrderedDict()
        sqlite_weibo["user_id"] = weibo["user_id"]
        sqlite_weibo["id"] = weibo["id"]
        sqlite_weibo["bid"] = weibo["bid"]
        sqlite_weibo["screen_name"] = weibo["screen_name"]
        sqlite_weibo["text"] = weibo["text"]
        sqlite_weibo["article_url"] = weibo["article_url"]
        sqlite_weibo["topics"] = weibo["topics"]
        sqlite_weibo["pics"] = weibo["pics"]
        sqlite_weibo["video_url"] = weibo["video_url"]
        sqlite_weibo["live_photo_url"] = weibo["live_photo_url"]
        sqlite_weibo["location"] = weibo["location"]
        sqlite_weibo["created_at"] = weibo["full_created_at"]
        sqlite_weibo["source"] = weibo["source"]
        sqlite_weibo["attitudes_count"] = weibo["attitudes_count"]
        sqlite_weibo["comments_count"] = weibo["comments_count"]
        sqlite_weibo["reposts_count"] = weibo["reposts_count"]
        sqlite_weibo["retweet_id"] = weibo["retweet_id"]
        sqlite_weibo["at_users"] = weibo["at_users"]
        sqlite_weibo["edited"] = weibo.get("edited", False)
        sqlite_weibo["edit_count"] = weibo.get("edit_count", 0)
        return sqlite_weibo

    def user_to_sqlite(self):
        con = self.get_sqlite_connection()
        self.sqlite_insert_user(con, self.user)
        con.close()

    def sqlite_insert_user(self, con: sqlite3.Connection, user: dict):
        sqlite_user = self.parse_sqlite_user(user)
        self.sqlite_insert(con, sqlite_user, "user")

    def parse_sqlite_user(self, user):
        if not user:
            return
        sqlite_user = OrderedDict()
        sqlite_user["id"] = user["id"]
        sqlite_user["nick_name"] = user["screen_name"]
        sqlite_user["gender"] = user["gender"]
        sqlite_user["follower_count"] = user["followers_count"]
        sqlite_user["follow_count"] = user["follow_count"]
        sqlite_user["birthday"] = user["birthday"]
        sqlite_user["location"] = user["location"]
        sqlite_user["ip_location"] = user.get("ip_location", "")         
        sqlite_user["edu"] = user["education"]
        sqlite_user["company"] = user["company"]
        sqlite_user["reg_date"] = user["registration_time"]
        sqlite_user["main_page_url"] = user["profile_url"]
        sqlite_user["avatar_url"] = user["avatar_hd"]
        sqlite_user["bio"] = user["description"]
        return sqlite_user

    def sqlite_insert(self, con: sqlite3.Connection, data: dict, table: str):
        if not data:
            return
        cur = con.cursor()
        keys = ",".join(data.keys())
        values = ",".join(["?"] * len(data))
        sql = """INSERT OR REPLACE INTO {table}({keys}) VALUES({values})
                """.format(
            table=table, keys=keys, values=values
        )
        cur.execute(sql, list(data.values()))
        con.commit()

    def get_sqlite_connection(self):
        path = self.get_sqlte_path()
        create = False
        if not os.path.exists(path):
            create = True

        con = sqlite3.connect(path)

        if create == True:
            self.create_sqlite_table(connection=con)

        return con

    def create_sqlite_table(self, connection: sqlite3.Connection):
        sql = self.get_sqlite_create_sql()
        cur = connection.cursor()
        cur.executescript(sql)
        connection.commit()

    def get_sqlte_path(self):
        return "./weibo/weibodata.db"

    def get_sqlite_create_sql(self):
        create_sql = """
                CREATE TABLE IF NOT EXISTS user (
                    id varchar(64) NOT NULL
                    ,nick_name varchar(64) NOT NULL
                    ,gender varchar(6)
                    ,follower_count integer
                    ,follow_count integer
                    ,birthday varchar(10)
                    ,location varchar(32)
                    ,ip_location varchar(32)
                    ,edu varchar(32)
                    ,company varchar(32)
                    ,reg_date DATETIME
                    ,main_page_url text
                    ,avatar_url text
                    ,bio text
                    ,PRIMARY KEY (id)
                );

                CREATE TABLE IF NOT EXISTS weibo (
                    id varchar(20) NOT NULL
                    ,bid varchar(12) NOT NULL
                    ,user_id varchar(20)
                    ,screen_name varchar(30)
                    ,text varchar(2000)
                    ,article_url varchar(100)
                    ,topics varchar(200)
                    ,at_users varchar(1000)
                    ,pics varchar(3000)
                    ,video_url varchar(1000)
                    ,live_photo_url varchar(1000)
                    ,location varchar(100)
                    ,created_at DATETIME
                    ,source varchar(30)
                    ,attitudes_count INT
                    ,comments_count INT
                    ,reposts_count INT
                    ,retweet_id varchar(20)
                    ,edited BOOLEAN DEFAULT 0
                    ,edit_count INT DEFAULT 0                    
                    ,PRIMARY KEY (id)
                );

                CREATE TABLE IF NOT EXISTS bins (
                    id integer PRIMARY KEY AUTOINCREMENT
                    ,ext varchar(10) NOT NULL /*file extension*/
                    ,data blob NOT NULL
                    ,weibo_id varchar(20)
                    ,comment_id varchar(20)
                    ,path text
                    ,url text
                );

                CREATE TABLE IF NOT EXISTS comments (
                    id varchar(20) NOT NULL
                    ,bid varchar(20) NOT NULL
                    ,weibo_id varchar(32) NOT NULL
                    ,root_id varchar(20) 
                    ,user_id varchar(20) NOT NULL
                    ,created_at varchar(20)
                    ,user_screen_name varchar(64) NOT NULL
                    ,user_avatar_url text
                    ,text varchar(1000)
                    ,pic_url text
                    ,like_count integer
                    ,PRIMARY KEY (id)
                );

                CREATE TABLE IF NOT EXISTS reposts (
                    id varchar(20) NOT NULL
                    ,bid varchar(20) NOT NULL
                    ,weibo_id varchar(32) NOT NULL
                    ,user_id varchar(20) NOT NULL
                    ,created_at varchar(20)
                    ,user_screen_name varchar(64) NOT NULL
                    ,user_avatar_url text
                    ,text varchar(1000)
                    ,like_count integer
                    ,PRIMARY KEY (id)
                );
                """
        return create_sql

    def update_user_config_file(self, user_config_file_path):
        """æ›´æ–°ç”¨æˆ·é…ç½®æ–‡ä»¶"""
        with open(user_config_file_path, "rb") as f:
            try:
                lines = f.read().splitlines()
                lines = [line.decode("utf-8-sig") for line in lines]
            except UnicodeDecodeError:
                logger.error("%sæ–‡ä»¶åº”ä¸ºutf-8ç¼–ç ï¼Œè¯·å…ˆå°†æ–‡ä»¶ç¼–ç è½¬ä¸ºutf-8å†è¿è¡Œç¨‹åº", user_config_file_path)
                sys.exit()
            for i, line in enumerate(lines):
                info = line.split(" ")
                if len(info) > 0 and info[0].isdigit():
                    if self.user_config["user_id"] == info[0]:
                        if len(info) == 1:
                            info.append(self.user["screen_name"])
                            info.append(self.start_date)
                        if len(info) == 2:
                            info.append(self.start_date)
                        if len(info) > 2:
                            info[2] = self.start_date
                        lines[i] = " ".join(info)
                        break
        with codecs.open(user_config_file_path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))

    def write_markdown(self, wrote_count):
        """å°†çˆ¬åˆ°çš„ä¿¡æ¯å†™å…¥markdownæ–‡ä»¶"""
        # æŒ‰æ—¥æœŸåˆ†ç»„å¾®åš
        weibo_by_date = self.group_weibo_by_date(wrote_count)

        # å…ˆä¸‹è½½å›¾ç‰‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.original_pic_download:
            self.download_markdown_images(wrote_count)

        # ä¸ºæ¯ä¸ªæ—¥æœŸç”Ÿæˆmarkdownæ–‡ä»¶
        for date, weibo_list in weibo_by_date.items():
            self.generate_markdown_file(date, weibo_list)

        logger.info("%dæ¡å¾®åšå†™å…¥markdownæ–‡ä»¶å®Œæ¯•", self.got_count - wrote_count)

    def group_weibo_by_date(self, wrote_count):
        """æŒ‰æ—¥æœŸåˆ†ç»„å¾®åš"""
        weibo_by_date = {}
        for w in self.weibo[wrote_count:]:
            # è·å–å¾®åšå‘å¸ƒæ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
            created_at = w.get("created_at", "")
            if not created_at:
                continue

            # è§£ææ—¥æœŸï¼Œæå– YYYY-MM-DD éƒ¨åˆ†
            try:
                date_obj = datetime.strptime(created_at, DTFORMAT)
                date_str = date_obj.strftime("%Y-%m-%d")

                if date_str not in weibo_by_date:
                    weibo_by_date[date_str] = []
                weibo_by_date[date_str].append(w)
            except ValueError:
                logger.warning(f"æ— æ³•è§£æå¾®åšæ—¥æœŸ: {created_at}")
                continue

        return weibo_by_date

    def download_markdown_images(self, wrote_count):
        """ä¸ºMarkdownæ ¼å¼ä¸‹è½½å›¾ç‰‡ï¼Œä½¿ç”¨æŒ‡å®šçš„å‘½åè§„åˆ™"""
        # è·å–ç”¨æˆ·ç›®å½•
        file_dir = self.get_filepath("markdown")
        img_dir = os.path.join(file_dir, "img")
        if not os.path.isdir(img_dir):
            os.makedirs(img_dir)

        # ä¸‹è½½å›¾ç‰‡
        for w in self.weibo[wrote_count:]:
            # å¤„ç†åŸåˆ›å¾®åšå›¾ç‰‡
            if w.get("pics"):
                self._download_weibo_images(w, img_dir, is_retweet=False)

            # å¤„ç†è½¬å‘å¾®åšå›¾ç‰‡
            if not self.only_crawl_original and w.get("retweet"):
                retweet = w["retweet"]
                if retweet.get("pics"):
                    self._download_weibo_images(retweet, img_dir, is_retweet=True)

    def _download_weibo_images(self, weibo, img_dir, is_retweet=False):
        """ä¸‹è½½å•æ¡å¾®åšçš„å›¾ç‰‡"""
        created_at = weibo.get("created_at", "")
        if not created_at:
            return

        try:
            time_obj = datetime.strptime(created_at, DTFORMAT)
            date_str = time_obj.strftime("%Y-%m-%d")
            time_str = time_obj.strftime("%H:%M:%S")
        except ValueError:
            return

        pics = weibo["pics"].split(",")
        for i, pic_url in enumerate(pics):
            if not pic_url:
                continue

            # ç”Ÿæˆå›¾ç‰‡æ–‡ä»¶åï¼šYYYY-MM-DD_HH-MM-SS.jpg
            # å¦‚æœåŒä¸€æ¡å¾®åšæœ‰å¤šå¼ å›¾ç‰‡ï¼Œåœ¨æ–‡ä»¶åååŠ  _1, _2 ç­‰åç¼€
            base_filename = f"{date_str}_{time_str.replace(':', '-')}"
            if len(pics) > 1:
                img_filename = f"{base_filename}_{i+1}.jpg"
            else:
                img_filename = f"{base_filename}.jpg"

            img_path = os.path.join(img_dir, img_filename)

            # ä¸‹è½½å›¾ç‰‡
            self.download_one_file(pic_url, img_path, "img", weibo["id"], created_at)

    def generate_markdown_file(self, date, weibo_list):
        """ç”Ÿæˆå•ä¸ªæ—¥æœŸçš„markdownæ–‡ä»¶ï¼ˆå¢é‡æ¨¡å¼ï¼‰"""
        # è·å–ç”¨æˆ·ç›®å½•
        file_dir = self.get_filepath("markdown")

        # åˆ›å»ºmarkdownæ–‡ä»¶è·¯å¾„
        md_file_path = os.path.join(file_dir, f"{date}.md")

        # è·å–ç”¨æˆ·å
        username = self.user.get("screen_name", "æœªçŸ¥ç”¨æˆ·")

        # è¯»å–å·²æœ‰æ–‡ä»¶ä¸­çš„å¾®åšIDï¼Œç”¨äºå»é‡ï¼ˆæ¯”æ—¶é—´æˆ³æ›´å¯é ï¼‰
        existing_weibo_ids = set()
        existing_content = ""
        if os.path.exists(md_file_path):
            try:
                with open(md_file_path, "r", encoding="utf-8") as f:
                    existing_content = f.read()
                    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ‰€æœ‰ <!-- weibo_id: xxx --> æ ¼å¼çš„å¾®åšID
                    weibo_id_pattern = r"<!-- weibo_id: (\d+) -->"
                    matches = re.findall(weibo_id_pattern, existing_content)
                    existing_weibo_ids = set(matches)
                logger.info(f"å·²è¯»å–ç°æœ‰MDæ–‡ä»¶ï¼ŒåŒ…å« {len(existing_weibo_ids)} æ¡å¾®åšè®°å½•")
            except Exception as e:
                logger.warning(f"è¯»å–ç°æœ‰MDæ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
                existing_content = ""
                existing_weibo_ids = set()

        # è¿‡æ»¤å‡ºæ–°çš„å¾®åšï¼ˆä¸åœ¨å·²æœ‰æ–‡ä»¶ä¸­çš„ï¼‰
        new_weibo_list = []
        for w in weibo_list:
            weibo_id = str(w.get("id", ""))
            if weibo_id and weibo_id not in existing_weibo_ids:
                new_weibo_list.append(w)

        # å¦‚æœæ²¡æœ‰æ–°å¾®åšï¼Œç›´æ¥è¿”å›
        if not new_weibo_list:
            logger.info(f"æ—¥æœŸ {date} æ²¡æœ‰æ–°å¾®åšéœ€è¦å†™å…¥")
            return

        # æ„å»ºæ–°å¾®åšçš„markdownå†…å®¹
        new_md_content = ""
        for w in new_weibo_list:
            # è·å–æ—¶é—´ï¼ˆHH:MM:SSæ ¼å¼ï¼‰
            created_at = w.get("created_at", "")
            if not created_at:
                continue

            try:
                time_obj = datetime.strptime(created_at, DTFORMAT)
                time_str = time_obj.strftime("%H:%M:%S")
                date_str = time_obj.strftime("%Y-%m-%d")
            except ValueError:
                time_str = "00:00:00"
                date_str = date

            # æ·»åŠ æ—¶é—´æ ‡é¢˜å’Œå¾®åšIDï¼ˆç”¨äºå¢é‡æ¨¡å¼å»é‡ï¼‰
            weibo_id = w.get("id", "")
            new_md_content += f"### {time_str}\n<!-- weibo_id: {weibo_id} -->\n"

            # å¤„ç†è½¬å‘å¾®åš
            if not self.only_crawl_original and w.get("retweet"):
                # åŸåˆ›éƒ¨åˆ†
                text = w.get("text", "").strip()
                if text:
                    new_md_content += f"{text}\n\n"

                # è½¬å‘éƒ¨åˆ†
                retweet = w["retweet"]
                retweet_text = retweet.get("text", "").strip()
                if retweet_text:
                    new_md_content += f"> è½¬å‘: {retweet_text}\n\n"

                # è½¬å‘å¾®åšå›¾ç‰‡
                if retweet.get("pics"):
                    pics = retweet["pics"].split(",")
                    # ä½¿ç”¨è½¬å‘å¾®åšçš„æ—¶é—´
                    retweet_created_at = retweet.get("created_at", created_at)
                    try:
                        retweet_time_obj = datetime.strptime(retweet_created_at, DTFORMAT)
                        retweet_date_str = retweet_time_obj.strftime("%Y-%m-%d")
                        retweet_time_str = retweet_time_obj.strftime("%H:%M:%S")
                    except ValueError:
                        retweet_date_str = date_str
                        retweet_time_str = time_str

                    for i, pic_url in enumerate(pics):
                        if pic_url:
                            base_filename = f"{retweet_date_str}_{retweet_time_str.replace(':', '-')}"
                            if len(pics) > 1:
                                img_filename = f"{base_filename}_{i+1}.jpg"
                            else:
                                img_filename = f"{base_filename}.jpg"
                            new_md_content += f"![image](img/{img_filename})\n\n"
            else:
                # åŸåˆ›å¾®åš
                text = w.get("text", "").strip()
                if text:
                    new_md_content += f"{text}\n\n"

                # åŸåˆ›å¾®åšå›¾ç‰‡
                if w.get("pics"):
                    pics = w["pics"].split(",")
                    for i, pic_url in enumerate(pics):
                        if pic_url:
                            base_filename = f"{date_str}_{time_str.replace(':', '-')}"
                            if len(pics) > 1:
                                img_filename = f"{base_filename}_{i+1}.jpg"
                            else:
                                img_filename = f"{base_filename}.jpg"
                            new_md_content += f"![image](img/{img_filename})\n\n"

            # æ·»åŠ åˆ†éš”çº¿
            new_md_content += "---\n\n"

        # å†™å…¥æ–‡ä»¶ï¼ˆå¢é‡æ¨¡å¼ï¼‰
        try:
            if existing_content:
                # è¿½åŠ åˆ°å·²æœ‰å†…å®¹æœ«å°¾
                final_content = existing_content.rstrip() + "\n\n" + new_md_content
            else:
                # åˆ›å»ºæ–°æ–‡ä»¶ï¼Œæ·»åŠ æ ‡é¢˜
                final_content = f"## {date} [{username}] å¾®åšå­˜æ¡£\n\n" + new_md_content

            with open(md_file_path, "w", encoding="utf-8") as f:
                f.write(final_content)
            logger.info(f"Markdownæ–‡ä»¶å·²æ›´æ–°: {md_file_path}ï¼Œæ–°å¢ {len(new_weibo_list)} æ¡å¾®åš")
        except Exception as e:
            logger.error(f"ç”ŸæˆMarkdownæ–‡ä»¶å¤±è´¥: {e}")

    def write_data(self, wrote_count):
        """å°†çˆ¬åˆ°çš„ä¿¡æ¯å†™å…¥æ–‡ä»¶æˆ–æ•°æ®åº“"""
        if self.got_count > wrote_count:
            if "csv" in self.write_mode:
                self.write_csv(wrote_count)
            if "json" in self.write_mode:
                self.write_json(wrote_count)
            if "post" in self.write_mode:
                self.write_post(wrote_count)
            if "mysql" in self.write_mode:
                self.weibo_to_mysql(wrote_count)
            if "mongo" in self.write_mode:
                self.weibo_to_mongodb(wrote_count)
            if "sqlite" in self.write_mode:
                self.weibo_to_sqlite(wrote_count)
            if "markdown" in self.write_mode:
                self.write_markdown(wrote_count)

            # å›¾ç‰‡ä¸‹è½½é€»è¾‘ï¼šå¦‚æœä½¿ç”¨markdownæ¨¡å¼ï¼Œå›¾ç‰‡å·²åœ¨write_markdownä¸­ä¸‹è½½
            # å¦åˆ™æŒ‰åŸæœ‰é€»è¾‘ä¸‹è½½
            if self.original_pic_download and "markdown" not in self.write_mode:
                self.download_files("img", "original", wrote_count)
            if self.original_video_download:
                self.download_files("video", "original", wrote_count)
            if self.original_live_photo_download:
                self.download_files("live_photo", "original", wrote_count)
            # ä¸‹è½½è½¬å‘å¾®åšæ–‡ä»¶ï¼ˆå¦‚æœä¸ç¦çˆ¬è½¬å‘ï¼‰
            if not self.only_crawl_original:
                if self.retweet_pic_download and "markdown" not in self.write_mode:
                    self.download_files("img", "retweet", wrote_count)
                if self.retweet_video_download:
                    self.download_files("video", "retweet", wrote_count)
                if self.retweet_live_photo_download:
                    self.download_files("live_photo", "retweet", wrote_count)

    def get_pages(self):
        """è·å–å…¨éƒ¨å¾®åš"""
        try:
            # ç”¨æˆ·idä¸å¯ç”¨
            if self.get_user_info() != 0:
                return
            logger.info("å‡†å¤‡æœé›† {} çš„å¾®åš".format(self.user["screen_name"]))

            # é˜²å°ç¦ï¼šåˆå§‹åŒ–çˆ¬å–ç»Ÿè®¡
            if self.anti_ban_enabled:
                self.crawl_stats["start_time"] = time.time()
                cfg = self.anti_ban_config
                logger.info("ğŸ›¡ï¸ é˜²å°ç¦æ¨¡å¼å·²å¯ç”¨")
                logger.info("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
                logger.info("â”‚ æ¯ä¼šè¯æœ€å¤§å¾®åšæ•°: %-17dâ”‚", cfg['max_weibo_per_session'])
                logger.info("â”‚ æ‰¹æ¬¡å¤§å°: %-8d æ‰¹æ¬¡å»¶è¿Ÿ: %3dç§’ â”‚", cfg['batch_size'], cfg['batch_delay'])
                logger.info("â”‚ è¯·æ±‚å»¶è¿Ÿ: %d-%dç§’                   â”‚", cfg['request_delay_min'], cfg['request_delay_max'])
                logger.info("â”‚ æœ€å¤§ä¼šè¯æ—¶é—´: %-7dç§’            â”‚", cfg['max_session_time'])
                logger.info("â”‚ æœ€å¤§APIé”™è¯¯æ•°: %-20dâ”‚", cfg['max_api_errors'])
                logger.info("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

            if const.MODE == "append" and (
                "first_crawler" not in self.__dict__ or self.first_crawler is False
            ):
                # æœ¬æ¬¡è¿è¡Œçš„æŸç”¨æˆ·é¦–æ¬¡æŠ“å–ï¼Œç”¨äºæ ‡è®°æœ€æ–°çš„å¾®åšid
                self.first_crawler = True
            since_date = datetime.strptime(self.user_config["since_date"], DTFORMAT)
            today = datetime.today()
            if since_date <= today:    # since_date è‹¥ä¸ºæœªæ¥åˆ™æ— éœ€æ‰§è¡Œ
                page_count = self.get_page_count()
                wrote_count = 0
                page1 = 0
                random_pages = random.randint(1, 5)
                self.start_date = datetime.now().strftime(DTFORMAT)
                pages = range(self.start_page, page_count + 1)
                for page in tqdm(pages, desc="Progress"):
                    is_end = self.get_one_page(page)
                    
                    # é˜²å°ç¦ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦ä¼‘æ¯
                    if is_end == "need_rest":
                        # å…ˆå†™å…¥å·²çˆ¬å–çš„æ•°æ®
                        self.write_data(wrote_count)
                        wrote_count = self.got_count
                        
                        # æ‰§è¡Œä¼‘æ¯
                        self.perform_anti_ban_rest()
                        
                        # é‡ç½®ç»Ÿè®¡ï¼Œç»§ç»­çˆ¬å–
                        self.reset_crawl_stats()
                        continue
                    
                    if is_end:
                        break

                    # é˜²å°ç¦ï¼šæ£€æŸ¥æ‰¹æ¬¡å»¶è¿Ÿ
                    if self.anti_ban_enabled:
                        self.check_batch_delay()

                    if page % 20 == 0:  # æ¯çˆ¬20é¡µå†™å…¥ä¸€æ¬¡æ–‡ä»¶
                        self.write_data(wrote_count)
                        wrote_count = self.got_count

                    # é˜²å°ç¦ï¼šä¿ç•™åŸæœ‰å»¶è¿Ÿé€»è¾‘ï¼Œä½†å¯æ ¹æ®é…ç½®è°ƒæ•´
                    if self.anti_ban_enabled:
                        # å¦‚æœå¯ç”¨äº†é˜²å°ç¦ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„å»¶è¿Ÿ
                        if (page - page1) % random_pages == 0 and page < page_count:
                            delay = random.randint(8, 12)  # æ›´ä¿å®ˆçš„å»¶è¿Ÿ
                            sleep(delay)
                            page1 = page
                            random_pages = random.randint(1, 5)
                    else:
                        # åŸæœ‰é€»è¾‘
                        if (page - page1) % random_pages == 0 and page < page_count:
                            sleep(random.randint(6, 10))
                            page1 = page
                            random_pages = random.randint(1, 5)

                self.write_data(wrote_count)  # å°†å‰©ä½™ä¸è¶³20é¡µçš„å¾®åšå†™å…¥æ–‡ä»¶

            # é˜²å°ç¦ï¼šè¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            if self.anti_ban_enabled:
                session_time = time.time() - self.crawl_stats["start_time"]
                logger.info(f"é˜²å°ç¦ç»Ÿè®¡: å¾®åš={self.crawl_stats['weibo_count']}, è¯·æ±‚={self.crawl_stats['request_count']}, é”™è¯¯={self.crawl_stats['api_errors']}, è€—æ—¶={int(session_time)}ç§’")

            logger.info("å¾®åšçˆ¬å–å®Œæˆï¼Œå…±çˆ¬å–%dæ¡å¾®åš", self.got_count)
        except Exception as e:
            logger.exception(e)

    def get_user_config_list(self, file_path):
        """è·å–æ–‡ä»¶ä¸­çš„å¾®åšidä¿¡æ¯"""
        with open(file_path, "rb") as f:
            try:
                lines = f.read().splitlines() 
                lines = [line.decode("utf-8-sig") for line in lines]
            except UnicodeDecodeError:
                logger.error("%sæ–‡ä»¶åº”ä¸ºutf-8ç¼–ç ï¼Œè¯·å…ˆå°†æ–‡ä»¶ç¼–ç è½¬ä¸ºutf-8å†è¿è¡Œç¨‹åº", file_path)
                sys.exit()
            user_config_list = []
            # åˆ†è¡Œè§£æé…ç½®ï¼Œæ·»åŠ åˆ°user_config_list
            for line in lines:
                info = line.strip().split(" ")    # å»é™¤å­—ç¬¦ä¸²é¦–å°¾ç©ºç™½å­—ç¬¦
                if len(info) > 0 and info[0].isdigit():
                    user_config = {}
                    user_config["user_id"] = info[0]
                    # æ ¹æ®é…ç½®æ–‡ä»¶è¡Œçš„å­—æ®µæ•°ç¡®å®š since_date çš„å€¼
                    if len(info) == 3:
                        if self.is_datetime(info[2]):
                            user_config["since_date"] = info[2]
                        elif self.is_date(info[2]):
                            user_config["since_date"] = "{}T00:00:00".format(info[2])
                        elif info[2].isdigit():
                            since_date = date.today() - timedelta(int(info[2]))
                            user_config["since_date"] = since_date.strftime(DTFORMAT)
                        else:
                            logger.error("since_date æ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·ç¡®è®¤é…ç½®æ˜¯å¦æ­£ç¡®")
                            sys.exit()
                        logger.info(f"ç”¨æˆ· {user_config['user_id']} ä½¿ç”¨æ–‡ä»¶ä¸­çš„èµ·å§‹æ—¶é—´: {user_config['since_date']}")
                    else:
                        user_config["since_date"] = self.since_date
                        logger.info(f"ç”¨æˆ· {user_config['user_id']} ä½¿ç”¨é…ç½®æ–‡ä»¶çš„èµ·å§‹æ—¶é—´: {user_config['since_date']}")
                    # è‹¥è¶…è¿‡3ä¸ªå­—æ®µï¼Œåˆ™ç¬¬å››ä¸ªå­—æ®µä¸º query_list                    
                    if len(info) > 3:
                        user_config["query_list"] = info[3].split(",")
                    else:
                        user_config["query_list"] = self.query_list
                    if user_config not in user_config_list:
                        user_config_list.append(user_config)
        return user_config_list

    def initialize_info(self, user_config):
        """åˆå§‹åŒ–çˆ¬è™«ä¿¡æ¯"""
        self.weibo = []
        self.user = {}
        self.user_config = user_config
        self.got_count = 0
        self.weibo_id_list = []

    def start(self):
        """è¿è¡Œçˆ¬è™«"""
        try:
            for user_config in self.user_config_list:
                if len(user_config["query_list"]):
                    for query in user_config["query_list"]:
                        self.query = query
                        self.initialize_info(user_config)
                        self.get_pages()
                else:
                    self.initialize_info(user_config)
                    self.get_pages()

                # å½“å‰ç”¨æˆ·æ‰€æœ‰å¾®åšå’Œè¯„è®ºæŠ“å–å®Œæ¯•åï¼Œå†å¯¼å‡ºè¯¥ç”¨æˆ·çš„è¯„è®º CSV
                self.export_comments_to_csv_for_current_user()

                logger.info("ä¿¡æ¯æŠ“å–å®Œæ¯•")
                logger.info("*" * 100)
                if self.user_config_file_path and self.user:
                    self.update_user_config_file(self.user_config_file_path)
        except Exception as e:
            logger.exception(e)


def handle_config_renaming(config, oldName, newName):
    if oldName in config and newName not in config:
        config[newName] = config[oldName]
        del config[oldName]

def get_config():
    """è·å–é…ç½®æ–‡ä»¶ä¿¡æ¯ï¼ˆæ”¯æŒJSON5æ ¼å¼ï¼‰"""
    config_path = os.path.split(os.path.realpath(__file__))[0] + os.sep + "config.json"
    if not os.path.isfile(config_path):
        logger.warning(
            "å½“å‰è·¯å¾„ï¼š%s ä¸å­˜åœ¨é…ç½®æ–‡ä»¶config.json",
            (os.path.split(os.path.realpath(__file__))[0] + os.sep),
        )
        sys.exit()
    try:
        with open(config_path, encoding="utf-8") as f:
            config_content = f.read()
            # é¦–å…ˆå°è¯•ä½¿ç”¨JSON5è§£æï¼ˆæ”¯æŒæ³¨é‡Šï¼‰
            try:
                config = json5.loads(config_content)
            except Exception as json5_error:
                # å¦‚æœJSON5è§£æå¤±è´¥ï¼Œå°è¯•æ ‡å‡†JSONè§£æ
                try:
                    config = json.loads(config_content)
                    logger.info("ä½¿ç”¨æ ‡å‡†JSONæ ¼å¼è§£æé…ç½®æ–‡ä»¶")
                except Exception as json_error:
                    logger.error(f"JSON5è§£æå¤±è´¥: {json5_error}")
                    logger.error(f"æ ‡å‡†JSONè§£æä¹Ÿå¤±è´¥: {json_error}")
                    logger.error("é…ç½®æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·æ£€æŸ¥è¯­æ³•")
                    sys.exit()

            # é‡å‘½åä¸€äº›key, ä½†å‘å‰å…¼å®¹
            handle_config_renaming(config, oldName="filter", newName="only_crawl_original")
            handle_config_renaming(config, oldName="result_dir_name", newName="user_id_as_folder_name")
            return config
    except Exception as e:
        logger.error(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        logger.error("è¯·ç¡®ä¿config.jsonå­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®")
        sys.exit()


def main():
    try:
        config = get_config()
        wb = Weibo(config)
        wb.start()  # çˆ¬å–å¾®åšä¿¡æ¯
        if const.NOTIFY["NOTIFY"]:
            push_deer("æ›´æ–°äº†ä¸€æ¬¡å¾®åš")
    except Exception as e:
        if const.NOTIFY["NOTIFY"]:
            push_deer("weibo-crawlerè¿è¡Œå‡ºé”™ï¼Œé”™è¯¯ä¸º{}".format(e))
        logger.exception(e)


if __name__ == "__main__":
    main()
